{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a497268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c314c6",
   "metadata": {},
   "source": [
    "## Подготовка метадаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64319144",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_files = glob.glob('/mnt/d/Study/PhD/Data/EEG/*/*.set')\n",
    "df_eegs = pd.DataFrame(eeg_files, columns=['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2f13ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eegs['session_type'] = df_eegs['path'].apply(lambda x: x.split('/')[-2])\n",
    "df_eegs['mono'] = df_eegs['path'].apply(lambda x: 'mono' in x.split('/')[-1])\n",
    "df_eegs['subject_id'] = df_eegs['path'].apply(lambda x: '_'.join(x.split('/')[-1].replace('_mono', '').split('_')[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f05915a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "session_type\n",
       "fon      32\n",
       "other    32\n",
       "own      32\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eegs.session_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca8628a",
   "metadata": {},
   "source": [
    "## Подготовка кропов\n",
    "Подготовка датасета -- кропы с меткой открытых/закрытых глаз по аннотациям.\n",
    "\n",
    "Кропы фиксированной длины с пересечением, с отступом до и после аннотации сегмента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2c0de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ann_info(path, target_sfreq=500):\n",
    "    \"\"\"Load a .set file, return dict with path, total duration (s), ann.onset, ann.description.\"\"\"\n",
    "    try:\n",
    "        raw = mne.io.read_raw_eeglab(path, preload=True, verbose=False)\n",
    "        if raw.info[\"sfreq\"] != target_sfreq:\n",
    "            raw.resample(target_sfreq, verbose=False)\n",
    "        n_samples = raw.n_times\n",
    "        sfreq = raw.info[\"sfreq\"]\n",
    "        duration_sec = n_samples / sfreq\n",
    "        ann = raw.annotations\n",
    "\n",
    "        # cast onset and description to Python list for DataFrame compatibility\n",
    "        onset_list = ann.onset.tolist()\n",
    "        description_list = list(ann.description)\n",
    "        return {\n",
    "            'path': path,\n",
    "            'duration_sec': duration_sec,\n",
    "            'ann_onset': onset_list,\n",
    "            'ann_description': description_list\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'path': path,\n",
    "            'duration_sec': None,\n",
    "            'ann_onset': None,\n",
    "            'ann_description': None,\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48b01d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading EEGs with ann info:   0%|          | 0/96 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "Reading EEGs with ann info:  67%|██████▋   | 64/96 [00:11<00:05,  5.81it/s]/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "Reading EEGs with ann info: 100%|██████████| 96/96 [01:12<00:00,  1.32it/s]\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_2061/3587328136.py:4: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of  96 | elapsed:  2.5min finished\n"
     ]
    }
   ],
   "source": [
    "# получение метаинформации по сегментам (2.5 min / 96 eegs)\n",
    "records = Parallel(n_jobs=-1, verbose=1)(\n",
    "    delayed(extract_ann_info)(path, target_sfreq)\n",
    "    for path in tqdm(df_eegs['path'], desc=\"Reading EEGs with ann info\")\n",
    ")\n",
    "\n",
    "df_ann_info = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49e9cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ann_info['ann_description'] = df_ann_info['ann_description'].apply(lambda x: list(map(str, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1f5aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eegs = df_eegs.merge(df_ann_info, how='left', on='path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6872dc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eegs.to_parquet('../data/classification/raw_eegs.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dee1676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eegs = pd.read_parquet('../data/classification/raw_eegs.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2573d88",
   "metadata": {},
   "source": [
    "## Фильтрация сегментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8930bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ann_description to list of str and rename 'S 1'/'S 2' to 'S1'/'S2'\n",
    "def clean_ann_description(description):\n",
    "    # Handle None or NaN\n",
    "    if description is None:\n",
    "        return []\n",
    "    # If it's already a list, work with it\n",
    "    # If not, try to convert from numpy array or string repr\n",
    "    if isinstance(description, str):\n",
    "        # Try to eval as a list-like string\n",
    "        import ast\n",
    "        try:\n",
    "            description = ast.literal_eval(description)\n",
    "        except Exception:\n",
    "            description = [description]\n",
    "    # Convert to list for non-list types (e.g., numpy array)\n",
    "    desc_list = list(description)\n",
    "\n",
    "    # Fix string formatting\n",
    "    out = []\n",
    "    for d in desc_list:\n",
    "        d_str = str(d).strip()\n",
    "        if d_str in ['S 1', 'S   1']:\n",
    "            out.append('S1')\n",
    "        elif d_str in ['S 2', 'S   2']:\n",
    "            out.append('S2')\n",
    "        else:\n",
    "            out.append(d_str)\n",
    "    return out\n",
    "\n",
    "df_eegs['ann_description'] = df_eegs['ann_description'].apply(clean_ann_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a17cebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eegs['all_segments'] = df_eegs.apply(lambda row: [float(seg_start) for seg_start in row['ann_onset']] + [float(row['duration_sec'])], axis=1)\n",
    "df_eegs['segment_durations'] = df_eegs['all_segments'].apply(lambda x: [round(float(x[i+1]) - float(x[i]), 2) for i in range(len(x)-1)])\n",
    "def get_segments_by_type(row, seg_type):\n",
    "    segs = []\n",
    "    ann_desc = row['ann_description']\n",
    "    seg_starts = row['all_segments']\n",
    "    for i, desc in enumerate(ann_desc):\n",
    "        if desc == seg_type:\n",
    "            segs.append((seg_starts[i], seg_starts[i+1]))\n",
    "    return segs\n",
    "\n",
    "df_eegs['segments_S1'] = df_eegs.apply(lambda row: get_segments_by_type(row, 'S1'), axis=1)\n",
    "df_eegs['segments_S2'] = df_eegs.apply(lambda row: get_segments_by_type(row, 'S2'), axis=1)\n",
    "\n",
    "# all durations > 50 and < 130\n",
    "df_eegs['good_eeg'] = df_eegs['segment_durations'].apply(lambda x: all(50 < d < 150 for d in x))\n",
    "df_eegs['bad_segments'] = df_eegs['segment_durations'].apply(lambda x: [d for d in x if d < 50 or d > 130])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "403dbc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eegs.to_parquet('../data/classification/raw_eegs.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4183d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Сегменты\n",
    "# Make frame where each row is a S1 or S2 segment (with label and EEG metadata)\n",
    "\n",
    "segment_rows = []\n",
    "\n",
    "for idx, row in df_eegs.iterrows():\n",
    "    # S1 segments\n",
    "    for seg in row.get('segments_S1', []):\n",
    "        segment_rows.append({\n",
    "            'path': row['path'],\n",
    "            'session_type': row['session_type'],\n",
    "            'mono': row['mono'],\n",
    "            'subject_id': row.get('subject_id', None),\n",
    "            'duration_sec': row.get('duration_sec', None),\n",
    "            'good_eeg': row.get('good_eeg', None),\n",
    "            'segment_type': 'S1',\n",
    "            'segment_start': seg[0],\n",
    "            'segment_end': seg[1],\n",
    "            'segment_len': seg[1] - seg[0],\n",
    "            'bad_segments_for_eeg': row.get('bad_segments', []),\n",
    "        })\n",
    "    # S2 segments\n",
    "    for seg in row.get('segments_S2', []):\n",
    "        segment_rows.append({\n",
    "            'path': row['path'],\n",
    "            'session_type': row['session_type'],\n",
    "            'mono': row['mono'],\n",
    "            'subject_id': row.get('subject_id', None),\n",
    "            'duration_sec': row.get('duration_sec', None),\n",
    "            'good_eeg': row.get('good_eeg', None),\n",
    "            'segment_type': 'S2',\n",
    "            'segment_start': seg[0],\n",
    "            'segment_end': seg[1],\n",
    "            'segment_len': seg[1] - seg[0],\n",
    "            'bad_segments_for_eeg': row.get('bad_segments', []),\n",
    "        })\n",
    "\n",
    "df_segments = pd.DataFrame(segment_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "970c4cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segments.to_parquet('../data/classification/raw_eegs_segments.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0c0cbe",
   "metadata": {},
   "source": [
    "## Нарезка кропов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "832d96c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "CROPS_DIR = '../data/classification_crops'\n",
    "\n",
    "target_sfreq = 500  # Hz\n",
    "\n",
    "# sliding window params\n",
    "segment_sec = 5.0           # window length in seconds (crop length)\n",
    "overlap_sec = 2.0           # overlap in seconds\n",
    "segment_margin_sec = 5.0    # clean margin from event start/end in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9fd9eb87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c0d12ddedc4a4abef95d45d835978e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cropping EEGs (serial):   0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
      "/tmp/ipykernel_2061/3878956376.py:11: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def crop_segments_for_eeg(row, crops_dir=CROPS_DIR):\n",
    "    \"\"\"For each segment (S1/S2) in row, resample, crop into overlapping crops, save and return rows.\"\"\"\n",
    "    crop_rows = []\n",
    "    try:\n",
    "        raw = mne.io.read_raw_eeglab(row['path'], preload=True, verbose=False)\n",
    "        if raw.info['sfreq'] != target_sfreq:\n",
    "            raw.resample(target_sfreq)\n",
    "        # for both S1 and S2 segments:\n",
    "        for seg_type in ['segments_S1', 'segments_S2']:\n",
    "            segments = row.get(seg_type, [])\n",
    "            stype = 'S1' if seg_type == 'segments_S1' else 'S2'\n",
    "            for seg_start, seg_end in segments:\n",
    "                # apply margins\n",
    "                crop_start = seg_start + segment_margin_sec\n",
    "                crop_end = seg_end - segment_margin_sec\n",
    "                if crop_end <= crop_start:\n",
    "                    continue  # skip too small segments\n",
    "                t = crop_start\n",
    "                while t + segment_sec <= crop_end + 1e-5:  # allow fp error\n",
    "                    start_sample = int(t * target_sfreq)\n",
    "                    stop_sample = int((t + segment_sec) * target_sfreq)\n",
    "                    crop = raw.get_data(start=start_sample, stop=stop_sample)\n",
    "                    crop_id = f\"{os.path.splitext(os.path.basename(row['path']))[0]}_{stype}_{int(1000*t)}_{int(1000*(t+segment_sec))}.npy\"\n",
    "                    crop_path = os.path.join(crops_dir, crop_id)\n",
    "                    np.save(crop_path, crop.astype(np.float32))\n",
    "                    crop_rows.append({\n",
    "                        'crop_path': crop_path,\n",
    "                        'subject_id': row.get('subject_id', None),\n",
    "                        'session_type': row.get('session_type', None),\n",
    "                        'mono': row.get('mono', None),\n",
    "                        'segment_type': stype,\n",
    "                        'segment_orig_start': seg_start,\n",
    "                        'segment_orig_end': seg_end,\n",
    "                        'crop_start_time': t,\n",
    "                        'crop_end_time': t + segment_sec,\n",
    "                        'good_eeg': row.get('good_eeg', None),\n",
    "                    })\n",
    "                    t += segment_sec - overlap_sec\n",
    "    except Exception as e:\n",
    "        print(f\"Failed {row['path']}: {e}\")\n",
    "    return crop_rows\n",
    "\n",
    "os.makedirs(CROPS_DIR, exist_ok=True)\n",
    "\n",
    "all_crop_rows = []\n",
    "rows = df_eegs.to_dict('records')\n",
    "\n",
    "# Run serially to avoid pickling error with joblib/Parallel\n",
    "with tqdm(total=len(rows), desc=\"Cropping EEGs (serial)\") as pbar:\n",
    "    for row in rows:\n",
    "        result = crop_segments_for_eeg(row)\n",
    "        all_crop_rows.extend(result)\n",
    "        pbar.update(1)\n",
    "\n",
    "df_crops = pd.DataFrame(all_crop_rows)\n",
    "df_crops.to_parquet('../data/classification/raw_eegs_crops.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfdf4a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of segments: 6841\n",
      "First 3 crop paths:\n",
      " ['../data/classification_crops/Co_y6_003_fon1_S1_41292_46292.npy', '../data/classification_crops/Co_y6_003_fon1_S1_44292_49292.npy', '../data/classification_crops/Co_y6_003_fon1_S1_47292_52292.npy']\n",
      "EEG segments np array shape: (128, 128, 2500)\n",
      "Input torch tensor shape: torch.Size([128, 128, 2500])\n",
      "Model output shape: torch.Size([128, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataframe with crop segment paths\n",
    "df = pd.read_parquet('../data/classification/raw_eegs_crops.parquet')\n",
    "print(\"Number of segments:\", len(df))\n",
    "print(\"First 3 crop paths:\\n\", df['crop_path'].head(3).tolist())\n",
    "\n",
    "# Load a batch of EEG crops (e.g., first 8)\n",
    "batch_size = 128\n",
    "batch_paths = df['crop_path'].head(batch_size).tolist()\n",
    "\n",
    "eeg_segments = []\n",
    "for p in batch_paths:\n",
    "    d = np.load(p)  # shape: (n_channels, n_samples)\n",
    "    eeg_segments.append(d)\n",
    "eeg_segments = np.stack(eeg_segments)  # (batch_size, n_channels, n_samples)\n",
    "print(\"EEG segments np array shape:\", eeg_segments.shape)\n",
    "\n",
    "# Convert to torch tensor\n",
    "inputs = torch.from_numpy(eeg_segments.astype(np.float32))  # (batch, ch, sampl)\n",
    "print(\"Input torch tensor shape:\", inputs.shape)\n",
    "\n",
    "# Optionally move to cuda if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "# Import/load the model (see EEGNet from eda_eegs.ipynb)\n",
    "# For demonstration, assume n_channels and n_samples:\n",
    "n_channels = inputs.shape[1]\n",
    "in_samples = inputs.shape[2]\n",
    "n_classes = 2  # or as appropriate\n",
    "\n",
    "# Define model (copy-paste EEGNet or import if available)\n",
    "class EEGNet(torch.nn.Module):\n",
    "    def __init__(self, n_channels=128, in_samples=2500, n_classes=2):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(n_channels, 64, kernel_size=7, stride=1, padding=3)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(64)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2, groups=64)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(128)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 256, kernel_size=1)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(256)\n",
    "        self.pool = torch.nn.AdaptiveAvgPool1d(32)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.fc1 = torch.nn.Linear(256 * 32, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, n_classes)\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.nn.functional.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.nn.functional.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = EEGNet(n_channels=n_channels, in_samples=in_samples, n_classes=n_classes).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(inputs)\n",
    "print(\"Model output shape:\", out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "467987d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_crops' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_crops[\u001b[33m\"\u001b[39m\u001b[33mcrop_len\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf_crops\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mcrop_end_time\u001b[39m\u001b[33m\"\u001b[39m] - df_crops[\u001b[33m\"\u001b[39m\u001b[33mcrop_start_time\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'df_crops' is not defined"
     ]
    }
   ],
   "source": [
    "df_crops[\"crop_len\"] = df_crops[\"crop_end_time\"] - df_crops[\"crop_start_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa39a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "78b91e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статистика по шейпам .npy файлов (после коррекции):\n",
      "Shape (128, 2500): 6841 файлов\n",
      "Ось 0: min=128, max=128\n",
      "Ось 1: min=2500, max=2500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "# Папка с .npy-кропами\n",
    "crops_dir = \"../data/classification_crops/\"  # замените при необходимости\n",
    "\n",
    "DESIRED_CROP_LEN = 2500\n",
    "\n",
    "def load_and_fix_crop(path, desired_len=DESIRED_CROP_LEN, save_if_changed=True):\n",
    "    \"\"\"\n",
    "    Загружает .npy, корректирует длину до desired_len (обрезка или паддинг), сохраняет если изменено.\n",
    "    Возвращает итоговую форму.\n",
    "    \"\"\"\n",
    "    arr = np.load(path)\n",
    "    orig_shape = arr.shape\n",
    "    n_chans, n_samples = arr.shape\n",
    "\n",
    "    if n_samples == desired_len:\n",
    "        return orig_shape  # ок, ничего не делаем\n",
    "\n",
    "    # Исправляем\n",
    "    if n_samples > desired_len:\n",
    "        arr_fixed = arr[:, :desired_len]\n",
    "    else:  # n_samples < desired_len\n",
    "        pad_width = desired_len - n_samples\n",
    "        arr_fixed = np.pad(arr, ((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    # Сохраняем обратно, если нужно\n",
    "    if save_if_changed:\n",
    "        np.save(path, arr_fixed)\n",
    "    return arr_fixed.shape\n",
    "\n",
    "all_shapes = []\n",
    "crop_paths = []\n",
    "\n",
    "# Сбор кропов и проверка формы\n",
    "for root, _, files in os.walk(crops_dir):\n",
    "    for f in files:\n",
    "        if f.endswith('.npy'):\n",
    "            path = os.path.join(root, f)\n",
    "            try:\n",
    "                shape = load_and_fix_crop(path, DESIRED_CROP_LEN, save_if_changed=True)\n",
    "                all_shapes.append(shape)\n",
    "                crop_paths.append(path)\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при обработке {path}: {e}\")\n",
    "\n",
    "# Статистика форм после коррекции\n",
    "shape_counts = collections.Counter(all_shapes)\n",
    "\n",
    "print(\"Статистика по шейпам .npy файлов (после коррекции):\")\n",
    "for shape, count in shape_counts.items():\n",
    "    print(f\"Shape {shape}: {count} файлов\")\n",
    "\n",
    "if all_shapes:\n",
    "    arr_shapes = np.array(all_shapes)\n",
    "    for axis in range(arr_shapes.shape[1]):\n",
    "        min_dim = arr_shapes[:, axis].min()\n",
    "        max_dim = arr_shapes[:, axis].max()\n",
    "        print(f\"Ось {axis}: min={min_dim}, max={max_dim}\")\n",
    "else:\n",
    "    print(\"Файлы .npy не найдены в указанной директории.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c888919",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crops = pd.read_parquet('../data/classification/raw_eegs_crops.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e821aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уникальные значения df_crops['segment_type']: ['S1' 'S2']\n",
      "Распределение S1/S2:\n",
      "segment_type\n",
      "S1    3810\n",
      "S2    3031\n",
      "Name: count, dtype: int64\n",
      "Уникальные метки label:  [0 1]\n",
      "Распределение по label:\n",
      "label\n",
      "0    3810\n",
      "1    3031\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m df_crops[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m].isna().sum() == \u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mВ label есть пропуски!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Проверим, что в каждой записи crop_path существует\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m missing_crop_paths = \u001b[43mdf_crops\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcrop_path\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m n_missing = missing_crop_paths.sum()\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_missing > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/phd/lib/python3.13/site-packages/pandas/core/series.py:4943\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4810\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4815\u001b[39m     **kwargs,\n\u001b[32m   4816\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4817\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4818\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4819\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4934\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4935\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4937\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4941\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/phd/lib/python3.13/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/phd/lib/python3.13/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/phd/lib/python3.13/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/phd/lib/python3.13/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(p)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m df_crops[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m].isna().sum() == \u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mВ label есть пропуски!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Проверим, что в каждой записи crop_path существует\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m missing_crop_paths = df_crops[\u001b[33m\"\u001b[39m\u001b[33mcrop_path\u001b[39m\u001b[33m\"\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m.path.exists(p))\n\u001b[32m     30\u001b[39m n_missing = missing_crop_paths.sum()\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_missing > \u001b[32m0\u001b[39m:\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, f1_score\n",
    "\n",
    "# --- Блок проверки датасета S1/S2 и базовых свойств ---\n",
    "\n",
    "# Проверим уникальные значения segment_type\n",
    "print(\"Уникальные значения df_crops['segment_type']:\", df_crops[\"segment_type\"].unique())\n",
    "\n",
    "# Проверим распределение классов\n",
    "print(\"Распределение S1/S2:\")\n",
    "print(df_crops[\"segment_type\"].value_counts())\n",
    "\n",
    "# label = 0 для S1, label = 1 для S2\n",
    "label_map = {\"S1\": 0, \"S2\": 1}\n",
    "df_crops = df_crops.copy()\n",
    "df_crops[\"label\"] = df_crops[\"segment_type\"].map(label_map)\n",
    "\n",
    "# Убедимся, что все метки корректно проставлены\n",
    "print(\"Уникальные метки label: \", df_crops[\"label\"].unique())\n",
    "print(\"Распределение по label:\")\n",
    "print(df_crops[\"label\"].value_counts())\n",
    "\n",
    "# Проверим, что соответствие между segment_type и label без пропусков\n",
    "assert df_crops[\"label\"].isna().sum() == 0, \"В label есть пропуски!\"\n",
    "\n",
    "# Проверим, что в каждой записи crop_path существует\n",
    "missing_crop_paths = df_crops[\"crop_path\"].apply(lambda p: not os.path.exists(p))\n",
    "n_missing = missing_crop_paths.sum()\n",
    "if n_missing > 0:\n",
    "    print(f\"Внимание: {n_missing} файлов по crop_path не найдены!\")\n",
    "    print(df_crops[missing_crop_paths][[\"crop_path\", \"subject_id\", \"segment_type\"]])\n",
    "else:\n",
    "    print(\"Все crop_path существуют.\")\n",
    "\n",
    "# Проверим сколько уникальных subject_id:\n",
    "subj_counts = df_crops[\"subject_id\"].value_counts()\n",
    "print(f\"Уникальных subject_id: {df_crops['subject_id'].nunique()}\")\n",
    "print(\"Топ-5 субъектов по числу crop-ов:\")\n",
    "print(subj_counts.head())\n",
    "\n",
    "# --- Разделяем на train/val по subject_id ---\n",
    "unique_subjects = df_crops[\"subject_id\"].unique()\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(unique_subjects)\n",
    "train_subjects = unique_subjects[:23]\n",
    "val_subjects = unique_subjects[23:]\n",
    "\n",
    "print(f\"Train subjects:\\n{sorted(train_subjects)} (n={len(train_subjects)})\")\n",
    "print(f\"Val subjects:\\n{sorted(val_subjects)} (n={len(val_subjects)})\")\n",
    "\n",
    "train_mask = df_crops[\"subject_id\"].isin(train_subjects)\n",
    "val_mask = df_crops[\"subject_id\"].isin(val_subjects)\n",
    "df_train = df_crops[train_mask].reset_index(drop=True)\n",
    "df_val = df_crops[val_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train crops: {len(df_train)}; Val crops: {len(df_val)}\")\n",
    "\n",
    "# Проверим баланс классов в train и val\n",
    "print(\"Train распределение label:\")\n",
    "print(df_train[\"label\"].value_counts())\n",
    "print(\"Val распределение label:\")\n",
    "print(df_val[\"label\"].value_counts())\n",
    "\n",
    "# Дополнительно: не пересекаются ли subject_id в train/val?\n",
    "intersection = set(df_train[\"subject_id\"]).intersection(set(df_val[\"subject_id\"]))\n",
    "print(f\"Пересечение subject_id между train и val: {intersection}\")\n",
    "\n",
    "# --- Проверяем форму и данные в .npy ---\n",
    "def load_eeg_npy(path):\n",
    "    arr = np.load(path)\n",
    "    return arr\n",
    "\n",
    "# Сформируем shape-чеки\n",
    "X_train_shapes = []\n",
    "for path in df_train[\"crop_path\"]:\n",
    "    arr = load_eeg_npy(path)\n",
    "    X_train_shapes.append(arr.shape)\n",
    "X_val_shapes = []\n",
    "for path in df_val[\"crop_path\"]:\n",
    "    arr = load_eeg_npy(path)\n",
    "    X_val_shapes.append(arr.shape)\n",
    "\n",
    "print(\"Train shapes (уникальные):\", set(X_train_shapes))\n",
    "print(\"Val shapes (уникальные):\", set(X_val_shapes))\n",
    "\n",
    "# Берём только если все сэмплы одинаковой формы:\n",
    "if len(set(X_train_shapes)) == 1 and len(set(X_val_shapes)) == 1:\n",
    "    X_train = np.stack([load_eeg_npy(path) for path in df_train[\"crop_path\"]])\n",
    "    y_train = df_train[\"label\"].values\n",
    "    X_val = np.stack([load_eeg_npy(path) for path in df_val[\"crop_path\"]])\n",
    "    y_val = df_val[\"label\"].values\n",
    "\n",
    "    n_channels = X_train.shape[1]\n",
    "    in_samples = X_train.shape[2]\n",
    "    n_classes = 2\n",
    "    print(f\"Финальный X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"Финальный X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "else:\n",
    "    print(\"В train или val есть .npy разной формы! Проверьте исходные данные.\")\n",
    "\n",
    "# Дополнительно: sanity check — совмещаем label и содержимое\n",
    "print(\"Пример: перваые 5 меток train/val: \", y_train[:5], y_val[:5])\n",
    "print(\"Пример формы первого X_train:\", X_train[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d34fb2aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m     std = X.std(axis=(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m), keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m) + \u001b[32m1e-6\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (X - mean) / std\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m X_train_norm = channelwise_zscore(\u001b[43mX_train\u001b[49m)\n\u001b[32m     30\u001b[39m X_val_norm = channelwise_zscore(X_val)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mEEGDataset\u001b[39;00m(Dataset):\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Лосс растёт сразу с первой эпохи, а качество держится около 0.6. Давайте попробуем изменить тренировочный процесс:\n",
    "# 1. Попробуем уменьшить learning rate и batch size, чтобы оптимизация была стабильнее.\n",
    "# 2. Добавим нормализацию перед моделью (по каналам) вне DataLoader, на весь датасет.\n",
    "# 3. Упростим архитектуру: избавимся от агрессивного dropout, попробуем убрать логарифмирование/степени.\n",
    "# 4. Проведём sanity check: поменяем инициализацию последнего слоя на меньшие значения.\n",
    "# 5. Введём фиксированное seed для torch/numpy.\n",
    "# 6. Еще: scheduler зафризим, чтобы не мешал коротким датасетам.\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seed fix\n",
    "import random\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# 1. Глобальная пер-семпл нормализация X_train/X_val по каналам\n",
    "def channelwise_zscore(X):\n",
    "    # X: (N, ch, t)\n",
    "    mean = X.mean(axis=(0, 2), keepdims=True)\n",
    "    std = X.std(axis=(0, 2), keepdims=True) + 1e-6\n",
    "    return (X - mean) / std\n",
    "\n",
    "X_train_norm = channelwise_zscore(X_train)\n",
    "X_val_norm = channelwise_zscore(X_val)\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    \"\"\"EEG dataset без лишней batch-статистики; z-score каналов уже был применён.\"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx]), torch.tensor(self.y[idx])\n",
    "\n",
    "# 2. Модифицируем сеть: меньше dropout, проще pipeline\n",
    "class SimplerShallowConvNet(torch.nn.Module):\n",
    "    def __init__(self, n_channels=128, in_samples=2500, n_classes=2):\n",
    "        super().__init__()\n",
    "        self.conv_time = torch.nn.Conv1d(n_channels, 32, kernel_size=25, stride=1, padding=12, bias=False)\n",
    "        self.bn = torch.nn.BatchNorm1d(32)\n",
    "        self.pool = torch.nn.AvgPool1d(75, stride=15, padding=37)\n",
    "        self.dropout = torch.nn.Dropout(0.15)\n",
    "        # Вычислям выходной размер\n",
    "        pool_len = (in_samples + 2*37 - 75) // 15 + 1\n",
    "        self.fc = torch.nn.Linear(32 * pool_len, n_classes)\n",
    "        # Инициализация головы для лучших стартовых значений\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight, gain=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_time(x)\n",
    "        x = self.bn(x)\n",
    "        # Убираем степени и логарифм - это часто мешает градиентам на старте\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "batch_size = 64  # был 512 → уменьшаем для повторяемости/стабильности лосса\n",
    "train_ds = EEGDataset(X_train_norm, y_train)\n",
    "val_ds = EEGDataset(X_val_norm, y_val)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = SimplerShallowConvNet(n_channels=n_channels, in_samples=in_samples, n_classes=n_classes).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)  # был 1e-3\n",
    "# Отключаем scheduler (или делаем его очень терпеливым для диагностики)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1000)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"X_train_norm shape: {X_train_norm.shape}, X_val_norm shape: {X_val_norm.shape}\")\n",
    "print(f\"Train class counts: {np.bincount(y_train)}, Val class counts: {np.bincount(y_val)}\")\n",
    "print(\"Model summary:\")\n",
    "print(model)\n",
    "\n",
    "num_epochs = 30\n",
    "patience = 30\n",
    "best_bal_acc = 0\n",
    "epochs_no_improve = 0\n",
    "best_state_dict = None\n",
    "\n",
    "train_loss_hist, val_loss_hist = [], []\n",
    "train_acc_hist, val_acc_hist = [], []\n",
    "train_bal_acc_hist, val_bal_acc_hist = [], []\n",
    "train_f1_hist, val_f1_hist = [], []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    train_losses, all_train_preds, all_train_true = [], [], []\n",
    "\n",
    "    train_bar = tqdm(enumerate(train_dl), total=len(train_dl), desc=f\"Epoch {epoch} [Train]\", leave=False)\n",
    "    for batch_i, (xb, yb) in train_bar:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        preds = out.argmax(dim=1).detach().cpu().numpy()\n",
    "        all_train_preds.append(preds)\n",
    "        all_train_true.append(yb.detach().cpu().numpy())\n",
    "        train_bar.set_postfix({\"Batch loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    train_y_pred = np.concatenate(all_train_preds)\n",
    "    train_y_true = np.concatenate(all_train_true)\n",
    "    train_acc = accuracy_score(train_y_true, train_y_pred)\n",
    "    train_bal_acc = balanced_accuracy_score(train_y_true, train_y_pred)\n",
    "    train_f1 = f1_score(train_y_true, train_y_pred)\n",
    "    train_loss_epoch = np.mean(train_losses)\n",
    "    train_loss_hist.append(train_loss_epoch)\n",
    "    train_acc_hist.append(train_acc)\n",
    "    train_bal_acc_hist.append(train_bal_acc)\n",
    "    train_f1_hist.append(train_f1)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_val_preds, all_val_true = [], []\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(val_dl, total=len(val_dl), desc=f\"Epoch {epoch} [Val]\", leave=False)\n",
    "        for xb, yb in val_bar:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            val_loss = criterion(logits, yb)\n",
    "            val_losses.append(val_loss.item())\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            all_val_preds.append(preds)\n",
    "            all_val_true.append(yb.cpu().numpy())\n",
    "            val_bar.set_postfix({\"Batch loss\": f\"{val_loss.item():.4f}\"})\n",
    "\n",
    "    if len(all_val_preds) > 0:\n",
    "        val_y_pred = np.concatenate(all_val_preds)\n",
    "        val_y_true = np.concatenate(all_val_true)\n",
    "        bal_acc = balanced_accuracy_score(val_y_true, val_y_pred)\n",
    "        acc = accuracy_score(val_y_true, val_y_pred)\n",
    "        f1 = f1_score(val_y_true, val_y_pred)\n",
    "        val_loss_epoch = np.mean(val_losses)\n",
    "    else:\n",
    "        val_loss_epoch = bal_acc = acc = f1 = float('nan')\n",
    "\n",
    "    val_loss_hist.append(val_loss_epoch)\n",
    "    val_acc_hist.append(acc)\n",
    "    val_bal_acc_hist.append(bal_acc)\n",
    "    val_f1_hist.append(f1)\n",
    "\n",
    "    # Scheduler (no-op under patience=1000)\n",
    "    scheduler.step(val_loss_epoch)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch:2d} SUMMARY:\")\n",
    "    print(f\"  Train Loss: {train_loss_epoch:.4f} | Train Acc: {train_acc:.4f} | Train BA: {train_bal_acc:.4f} | Train F1: {train_f1:.4f}\")\n",
    "    print(f\"  Val   Loss: {val_loss_epoch:.4f} | Val   Acc: {acc:.4f} | Val   BA: {bal_acc:.4f} | Val   F1: {f1:.4f}\\n\")\n",
    "    print(f\"  Val confusion: {np.bincount(val_y_pred) if len(all_val_preds) else 'N/A'}\")\n",
    "\n",
    "    # Early stopping по валид баланс-acc\n",
    "    if bal_acc > best_bal_acc + 1e-4:\n",
    "        best_bal_acc = bal_acc\n",
    "        best_state_dict = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "        print(f\"  --> New best model (val balanced acc improved to {best_bal_acc:.4f})\\n\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping after {epoch} epochs (no val BA improvement for {patience} epochs).\")\n",
    "            break\n",
    "\n",
    "if best_state_dict:\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    print(f\"Best balanced accuracy on validation: {best_bal_acc:.4f}\")\n",
    "else:\n",
    "    print(\"No improvement on validation set.\")\n",
    "\n",
    "# --- Графики ---\n",
    "epochs_ran = len(train_loss_hist)\n",
    "fig, axs = plt.subplots(2, 2, figsize=(13, 9))\n",
    "\n",
    "axs[0,0].plot(range(1, epochs_ran+1), train_loss_hist, label='Train Loss')\n",
    "axs[0,0].plot(range(1, epochs_ran+1), val_loss_hist, label='Val Loss')\n",
    "axs[0,0].set_title(\"Loss\")\n",
    "axs[0,0].set_xlabel(\"Epoch\")\n",
    "axs[0,0].legend()\n",
    "axs[0,0].grid()\n",
    "\n",
    "axs[0,1].plot(range(1, epochs_ran+1), train_bal_acc_hist, label='Train BA')\n",
    "axs[0,1].plot(range(1, epochs_ran+1), val_bal_acc_hist, label='Val BA')\n",
    "axs[0,1].set_title(\"Balanced Accuracy\")\n",
    "axs[0,1].set_xlabel(\"Epoch\")\n",
    "axs[0,1].legend()\n",
    "axs[0,1].grid()\n",
    "\n",
    "axs[1,0].plot(range(1, epochs_ran+1), train_acc_hist, label='Train Acc')\n",
    "axs[1,0].plot(range(1, epochs_ran+1), val_acc_hist, label='Val Acc')\n",
    "axs[1,0].set_title(\"Accuracy\")\n",
    "axs[1,0].set_xlabel(\"Epoch\")\n",
    "axs[1,0].legend()\n",
    "axs[1,0].grid()\n",
    "\n",
    "axs[1,1].plot(range(1, epochs_ran+1), train_f1_hist, label='Train F1')\n",
    "axs[1,1].plot(range(1, epochs_ran+1), val_f1_hist, label='Val F1')\n",
    "axs[1,1].set_title(\"F1 Score\")\n",
    "axs[1,1].set_xlabel(\"Epoch\")\n",
    "axs[1,1].legend()\n",
    "axs[1,1].grid()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e9af08",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "39fd8280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_segments.subject_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a4bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b87c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e14c91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/mnt/d/Study/PhD/Data/EEG/own/Mor_y1_004_own_face.set',\n",
       " 'session_type': 'own',\n",
       " 'mono': np.False_,\n",
       " 'subject_id': 'Mor_y1_004',\n",
       " 'duration_sec': np.float64(746.172),\n",
       " 'ann_onset': array([  5.4095,   6.2615,   6.8955,  19.159 , 137.659 , 260.059 ,\n",
       "        378.359 , 498.859 , 618.359 ]),\n",
       " 'ann_description': array(['boundary', 'boundary', 'boundary', 'S1', 'S2', 'S1', 'S2', 'S1',\n",
       "        'S2'], dtype=object),\n",
       " 'all_segments': [5.4095,\n",
       "  6.2615,\n",
       "  6.8955,\n",
       "  19.159,\n",
       "  137.659,\n",
       "  260.059,\n",
       "  378.359,\n",
       "  498.859,\n",
       "  618.359,\n",
       "  746.172],\n",
       " 'segment_durations': [0.85,\n",
       "  0.63,\n",
       "  12.26,\n",
       "  118.5,\n",
       "  122.4,\n",
       "  118.3,\n",
       "  120.5,\n",
       "  119.5,\n",
       "  127.81],\n",
       " 'good_eeg': np.False_,\n",
       " 'bad_segments': [0.85, 0.63, 12.26],\n",
       " 'segments_S1': [(19.159, 137.659), (260.059, 378.359), (498.859, 618.359)],\n",
       " 'segments_S2': [(137.659, 260.059), (378.359, 498.859), (618.359, 746.172)]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(df_eegs.iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7473a81f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9bc87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>session_type</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>mono</th>\n",
       "      <th>duration_sec</th>\n",
       "      <th>ann_onset</th>\n",
       "      <th>ann_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_003_fon1.set</td>\n",
       "      <td>fon</td>\n",
       "      <td>Co_y6_003</td>\n",
       "      <td>False</td>\n",
       "      <td>599.032</td>\n",
       "      <td>[36.292, 132.311, 218.231, 305.396, 399.016, 4...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_008_fon1.set</td>\n",
       "      <td>fon</td>\n",
       "      <td>Co_y6_008</td>\n",
       "      <td>False</td>\n",
       "      <td>718.248</td>\n",
       "      <td>[8.361, 127.894, 244.88, 360.293, 478.137, 598...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_009_fon1.set</td>\n",
       "      <td>fon</td>\n",
       "      <td>Co_y6_009</td>\n",
       "      <td>False</td>\n",
       "      <td>756.598</td>\n",
       "      <td>[11.787, 126.276, 230.197, 339.358, 425.819, 5...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_010_fon1.set</td>\n",
       "      <td>fon</td>\n",
       "      <td>Co_y6_010</td>\n",
       "      <td>False</td>\n",
       "      <td>616.164</td>\n",
       "      <td>[27.844, 141.183, 252.179, 351.057, 422.676, 5...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_013_Fon1.set</td>\n",
       "      <td>fon</td>\n",
       "      <td>Co_y6_013</td>\n",
       "      <td>False</td>\n",
       "      <td>366.824</td>\n",
       "      <td>[2.02, 98.647, 143.365, 205.839, 244.698, 316....</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/own/Co_y6_mono_002_o...</td>\n",
       "      <td>own</td>\n",
       "      <td>Co_y6_002</td>\n",
       "      <td>True</td>\n",
       "      <td>669.910</td>\n",
       "      <td>[11.769, 115.769, 238.769, 357.769, 481.769, 5...</td>\n",
       "      <td>[S  1, S  2, S  1, S  2, S  1, S  2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/own/Mor_y1_001_own_f...</td>\n",
       "      <td>own</td>\n",
       "      <td>Mor_y1_001</td>\n",
       "      <td>False</td>\n",
       "      <td>731.932</td>\n",
       "      <td>[31.9455, 32.2, 148.3, 206.3135, 267.3, 274.86...</td>\n",
       "      <td>[boundary, S1, S2, boundary, S1, boundary, S2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/own/Mor_y1_002_own_f...</td>\n",
       "      <td>own</td>\n",
       "      <td>Mor_y1_002</td>\n",
       "      <td>False</td>\n",
       "      <td>676.492</td>\n",
       "      <td>[22.0, 134.0, 139.6125, 164.6385, 183.4675, 24...</td>\n",
       "      <td>[S1, S2, boundary, boundary, boundary, S1, bou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/own/Mor_y1_003_own_f...</td>\n",
       "      <td>own</td>\n",
       "      <td>Mor_y1_003</td>\n",
       "      <td>False</td>\n",
       "      <td>736.544</td>\n",
       "      <td>[8.6435, 11.8305, 17.712, 135.912, 256.912, 27...</td>\n",
       "      <td>[boundary, boundary, S1, S2, S1, boundary, bou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/own/Mor_y1_004_own_f...</td>\n",
       "      <td>own</td>\n",
       "      <td>Mor_y1_004</td>\n",
       "      <td>False</td>\n",
       "      <td>746.172</td>\n",
       "      <td>[5.4095, 6.2615, 6.8955, 19.159, 137.659, 260....</td>\n",
       "      <td>[boundary, boundary, boundary, S1, S2, S1, S2,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 path session_type  \\\n",
       "0    /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_003_fon1.set          fon   \n",
       "1    /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_008_fon1.set          fon   \n",
       "2    /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_009_fon1.set          fon   \n",
       "3    /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_010_fon1.set          fon   \n",
       "4    /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_013_Fon1.set          fon   \n",
       "..                                                ...          ...   \n",
       "91  /mnt/d/Study/PhD/Data/EEG/own/Co_y6_mono_002_o...          own   \n",
       "92  /mnt/d/Study/PhD/Data/EEG/own/Mor_y1_001_own_f...          own   \n",
       "93  /mnt/d/Study/PhD/Data/EEG/own/Mor_y1_002_own_f...          own   \n",
       "94  /mnt/d/Study/PhD/Data/EEG/own/Mor_y1_003_own_f...          own   \n",
       "95  /mnt/d/Study/PhD/Data/EEG/own/Mor_y1_004_own_f...          own   \n",
       "\n",
       "    subject_id   mono  duration_sec  \\\n",
       "0    Co_y6_003  False       599.032   \n",
       "1    Co_y6_008  False       718.248   \n",
       "2    Co_y6_009  False       756.598   \n",
       "3    Co_y6_010  False       616.164   \n",
       "4    Co_y6_013  False       366.824   \n",
       "..         ...    ...           ...   \n",
       "91   Co_y6_002   True       669.910   \n",
       "92  Mor_y1_001  False       731.932   \n",
       "93  Mor_y1_002  False       676.492   \n",
       "94  Mor_y1_003  False       736.544   \n",
       "95  Mor_y1_004  False       746.172   \n",
       "\n",
       "                                            ann_onset  \\\n",
       "0   [36.292, 132.311, 218.231, 305.396, 399.016, 4...   \n",
       "1   [8.361, 127.894, 244.88, 360.293, 478.137, 598...   \n",
       "2   [11.787, 126.276, 230.197, 339.358, 425.819, 5...   \n",
       "3   [27.844, 141.183, 252.179, 351.057, 422.676, 5...   \n",
       "4   [2.02, 98.647, 143.365, 205.839, 244.698, 316....   \n",
       "..                                                ...   \n",
       "91  [11.769, 115.769, 238.769, 357.769, 481.769, 5...   \n",
       "92  [31.9455, 32.2, 148.3, 206.3135, 267.3, 274.86...   \n",
       "93  [22.0, 134.0, 139.6125, 164.6385, 183.4675, 24...   \n",
       "94  [8.6435, 11.8305, 17.712, 135.912, 256.912, 27...   \n",
       "95  [5.4095, 6.2615, 6.8955, 19.159, 137.659, 260....   \n",
       "\n",
       "                                      ann_description  \n",
       "0          [S   1, S   2, S   1, S   2, S   1, S   2]  \n",
       "1          [S   1, S   2, S   1, S   2, S   1, S   2]  \n",
       "2          [S   1, S   2, S   1, S   2, S   1, S   2]  \n",
       "3          [S   1, S   2, S   1, S   2, S   1, S   2]  \n",
       "4          [S   1, S   2, S   1, S   2, S   1, S   2]  \n",
       "..                                                ...  \n",
       "91               [S  1, S  2, S  1, S  2, S  1, S  2]  \n",
       "92  [boundary, S1, S2, boundary, S1, boundary, S2,...  \n",
       "93  [S1, S2, boundary, boundary, boundary, S1, bou...  \n",
       "94  [boundary, boundary, S1, S2, S1, boundary, bou...  \n",
       "95  [boundary, boundary, boundary, S1, S2, S1, S2,...  \n",
       "\n",
       "[96 rows x 7 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# состояния глаз\n",
    "\n",
    "# 'S 1' -- closed\n",
    "# 'S 2' -- open\n",
    "\n",
    "# boundary -- skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b7362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
