{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eba5fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import mne\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f156459",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_files = glob.glob('/mnt/d/Study/PhD/Data/EEG/*/*.set')\n",
    "df_eegs = pd.DataFrame(eeg_files, columns=['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642b94ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_sfreq = 500  # Hz\n",
    "\n",
    "# Sliding window params\n",
    "segment_sec = 5.0           # window length in seconds (crop length)\n",
    "overlap_sec = 2.0           # overlap in seconds\n",
    "segment_margin_sec = 5.0      # clean margin from event start/end in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff155d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_file = '/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_041_fon1.set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd15d347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sfreq: 1000.0\n",
      "Resampled sfreq: 500.0\n"
     ]
    }
   ],
   "source": [
    "set_file = '/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_003_fon1.set'\n",
    "\n",
    "raw = mne.io.read_raw_eeglab(set_file, preload=True)\n",
    "print(\"Original sfreq:\", raw.info[\"sfreq\"])\n",
    "\n",
    "if raw.info[\"sfreq\"] != target_sfreq:\n",
    "    raw.resample(target_sfreq)\n",
    "    print(\"Resampled sfreq:\", raw.info[\"sfreq\"])\n",
    "else:\n",
    "    print(\"Already at target sfreq:\", raw.info[\"sfreq\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "198bb61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (128, 299516)\n"
     ]
    }
   ],
   "source": [
    "sfreq = raw.info[\"sfreq\"]\n",
    "data = raw.get_data()  # shape: (n_channels, n_samples)\n",
    "n_channels, n_samples = data.shape\n",
    "print(\"Data shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d23b5b56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599.032"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples / 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e54fb2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 36.292, 132.311, 218.231, 305.396, 399.016, 496.607]),\n",
       " array([0.001, 0.001, 0.001, 0.001, 0.001, 0.001]),\n",
       " array(['S   1', 'S   2', 'S   1', 'S   2', 'S   1', 'S   2'], dtype='<U5'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = raw.annotations\n",
    "ann.onset, ann.duration, ann.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7e2dbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading EEGs with ann info:   0%|          | 0/96 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "Reading EEGs with ann info:  67%|██████▋   | 64/96 [00:09<00:04,  6.52it/s]/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "Reading EEGs with ann info: 100%|██████████| 96/96 [01:11<00:00,  1.34it/s]\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "/tmp/ipykernel_3198/3617618697.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of  96 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>duration_sec</th>\n",
       "      <th>ann_onset</th>\n",
       "      <th>ann_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_003_fon1.set</td>\n",
       "      <td>599.032</td>\n",
       "      <td>[36.292, 132.311, 218.231, 305.396, 399.016, 4...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_008_fon1.set</td>\n",
       "      <td>718.248</td>\n",
       "      <td>[8.361, 127.894, 244.88, 360.293, 478.137, 598...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_009_fon1.set</td>\n",
       "      <td>756.598</td>\n",
       "      <td>[11.787, 126.276, 230.197, 339.358, 425.819, 5...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_010_fon1.set</td>\n",
       "      <td>616.164</td>\n",
       "      <td>[27.844, 141.183, 252.179, 351.057, 422.676, 5...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_013_Fon1.set</td>\n",
       "      <td>366.824</td>\n",
       "      <td>[2.02, 98.647, 143.365, 205.839, 244.698, 316....</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               path  duration_sec  \\\n",
       "0  /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_003_fon1.set       599.032   \n",
       "1  /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_008_fon1.set       718.248   \n",
       "2  /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_009_fon1.set       756.598   \n",
       "3  /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_010_fon1.set       616.164   \n",
       "4  /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_013_Fon1.set       366.824   \n",
       "\n",
       "                                           ann_onset  \\\n",
       "0  [36.292, 132.311, 218.231, 305.396, 399.016, 4...   \n",
       "1  [8.361, 127.894, 244.88, 360.293, 478.137, 598...   \n",
       "2  [11.787, 126.276, 230.197, 339.358, 425.819, 5...   \n",
       "3  [27.844, 141.183, 252.179, 351.057, 422.676, 5...   \n",
       "4  [2.02, 98.647, 143.365, 205.839, 244.698, 316....   \n",
       "\n",
       "                              ann_description  \n",
       "0  [S   1, S   2, S   1, S   2, S   1, S   2]  \n",
       "1  [S   1, S   2, S   1, S   2, S   1, S   2]  \n",
       "2  [S   1, S   2, S   1, S   2, S   1, S   2]  \n",
       "3  [S   1, S   2, S   1, S   2, S   1, S   2]  \n",
       "4  [S   1, S   2, S   1, S   2, S   1, S   2]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_ann_info(path, target_sfreq=500):\n",
    "    \"\"\"Load a .set file, return dict with path, total duration (s), ann.onset, ann.description.\"\"\"\n",
    "    try:\n",
    "        raw = mne.io.read_raw_eeglab(path, preload=True, verbose=False)\n",
    "        if raw.info[\"sfreq\"] != target_sfreq:\n",
    "            raw.resample(target_sfreq, verbose=False)\n",
    "        n_samples = raw.n_times\n",
    "        sfreq = raw.info[\"sfreq\"]\n",
    "        duration_sec = n_samples / sfreq\n",
    "        ann = raw.annotations\n",
    "        # Cast onset and description to Python list for DataFrame compatibility\n",
    "        onset_list = ann.onset.tolist()\n",
    "        description_list = list(ann.description)\n",
    "        return {\n",
    "            'path': path,\n",
    "            'duration_sec': duration_sec,\n",
    "            'ann_onset': onset_list,\n",
    "            'ann_description': description_list\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'path': path,\n",
    "            'duration_sec': None,\n",
    "            'ann_onset': None,\n",
    "            'ann_description': None,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Assuming df_eegs['path'] contains paths to EEG files\n",
    "records = Parallel(n_jobs=-1, verbose=1)(\n",
    "    delayed(extract_ann_info)(path, target_sfreq)\n",
    "    for path in tqdm(df_eegs['path'], desc=\"Reading EEGs with ann info\")\n",
    ")\n",
    "\n",
    "df_ann_info = pd.DataFrame(records)\n",
    "# If you want to see only successful cases (filter out errors if present):\n",
    "# df_ann_info = df_ann_info[df_ann_info[\"duration_sec\"].notnull()].reset_index(drop=True)\n",
    "df_ann_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fe465e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>duration_sec</th>\n",
       "      <th>ann_onset</th>\n",
       "      <th>ann_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_003_fon1.set</td>\n",
       "      <td>599.032</td>\n",
       "      <td>[36.292, 132.311, 218.231, 305.396, 399.016, 4...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_008_fon1.set</td>\n",
       "      <td>718.248</td>\n",
       "      <td>[8.361, 127.894, 244.88, 360.293, 478.137, 598...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_009_fon1.set</td>\n",
       "      <td>756.598</td>\n",
       "      <td>[11.787, 126.276, 230.197, 339.358, 425.819, 5...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_010_fon1.set</td>\n",
       "      <td>616.164</td>\n",
       "      <td>[27.844, 141.183, 252.179, 351.057, 422.676, 5...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_013_Fon1.set</td>\n",
       "      <td>366.824</td>\n",
       "      <td>[2.02, 98.647, 143.365, 205.839, 244.698, 316....</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_014_fon1.set</td>\n",
       "      <td>430.800</td>\n",
       "      <td>[2.422, 114.116, 223.771, 283.804, 330.991, 37...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_016st_fon1...</td>\n",
       "      <td>724.268</td>\n",
       "      <td>[7.869, 127.416, 247.423, 367.446, 485.62, 605...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_020_Fon1.set</td>\n",
       "      <td>658.736</td>\n",
       "      <td>[2.762, 115.947, 222.669, 335.911, 436.163, 55...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_021_fon1.set</td>\n",
       "      <td>727.936</td>\n",
       "      <td>[36.808, 150.913, 265.106, 382.931, 498.136, 6...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_022_fon1.set</td>\n",
       "      <td>680.956</td>\n",
       "      <td>[5.644, 118.733, 225.986, 342.245, 451.938, 57...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  duration_sec  \\\n",
       "0   /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_003_fon1.set       599.032   \n",
       "1   /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_008_fon1.set       718.248   \n",
       "2   /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_009_fon1.set       756.598   \n",
       "3   /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_010_fon1.set       616.164   \n",
       "4   /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_013_Fon1.set       366.824   \n",
       "5   /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_014_fon1.set       430.800   \n",
       "6  /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_016st_fon1...       724.268   \n",
       "7   /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_020_Fon1.set       658.736   \n",
       "8   /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_021_fon1.set       727.936   \n",
       "9   /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_022_fon1.set       680.956   \n",
       "\n",
       "                                           ann_onset  \\\n",
       "0  [36.292, 132.311, 218.231, 305.396, 399.016, 4...   \n",
       "1  [8.361, 127.894, 244.88, 360.293, 478.137, 598...   \n",
       "2  [11.787, 126.276, 230.197, 339.358, 425.819, 5...   \n",
       "3  [27.844, 141.183, 252.179, 351.057, 422.676, 5...   \n",
       "4  [2.02, 98.647, 143.365, 205.839, 244.698, 316....   \n",
       "5  [2.422, 114.116, 223.771, 283.804, 330.991, 37...   \n",
       "6  [7.869, 127.416, 247.423, 367.446, 485.62, 605...   \n",
       "7  [2.762, 115.947, 222.669, 335.911, 436.163, 55...   \n",
       "8  [36.808, 150.913, 265.106, 382.931, 498.136, 6...   \n",
       "9  [5.644, 118.733, 225.986, 342.245, 451.938, 57...   \n",
       "\n",
       "                              ann_description  \n",
       "0  [S   1, S   2, S   1, S   2, S   1, S   2]  \n",
       "1  [S   1, S   2, S   1, S   2, S   1, S   2]  \n",
       "2  [S   1, S   2, S   1, S   2, S   1, S   2]  \n",
       "3  [S   1, S   2, S   1, S   2, S   1, S   2]  \n",
       "4  [S   1, S   2, S   1, S   2, S   1, S   2]  \n",
       "5  [S   1, S   2, S   1, S   2, S   1, S   2]  \n",
       "6  [S   1, S   2, S   1, S   2, S   1, S   2]  \n",
       "7  [S   1, S   2, S   1, S   2, S   1, S   2]  \n",
       "8  [S   1, S   2, S   1, S   2, S   1, S   2]  \n",
       "9  [S   1, S   2, S   1, S   2, S   1, S   2]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ann_info[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85080418",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ann_info['all_segments'] = df_ann_info['ann_onset'].apply(lambda x: [float(seg_start) for seg_start in x] + [float(raw.duration)])\n",
    "df_ann_info['segment_durations'] = df_ann_info['all_segments'].apply(lambda x: [round(float(x[i+1]) - float(x[i]), 2) for i in range(len(x)-1)])\n",
    "\n",
    "# all durations > 50 and < 130\n",
    "df_ann_info['good_eeg'] = df_ann_info['segment_durations'].apply(lambda x: all(50 < d < 130 for d in x))\n",
    "df_ann_info['bad_segments'] = df_ann_info['segment_durations'].apply(lambda x: [d for d in x if d < 50 or d > 130])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6819c64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(23)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ann_info['good_eeg'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "470d47ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>duration_sec</th>\n",
       "      <th>ann_onset</th>\n",
       "      <th>ann_description</th>\n",
       "      <th>all_segments</th>\n",
       "      <th>segment_durations</th>\n",
       "      <th>good_eeg</th>\n",
       "      <th>bad_segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_008_fon1.set</td>\n",
       "      <td>718.248</td>\n",
       "      <td>[8.361, 127.894, 244.88, 360.293, 478.137, 598...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "      <td>[8.361, 127.894, 244.88, 360.293, 478.137, 598...</td>\n",
       "      <td>[119.53, 116.99, 115.41, 117.84, 120.02, 0.87]</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.87]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_013_Fon1.set</td>\n",
       "      <td>366.824</td>\n",
       "      <td>[2.02, 98.647, 143.365, 205.839, 244.698, 316....</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "      <td>[2.02, 98.647, 143.365, 205.839, 244.698, 316....</td>\n",
       "      <td>[96.63, 44.72, 62.47, 38.86, 71.58, 282.76]</td>\n",
       "      <td>False</td>\n",
       "      <td>[44.72, 38.86, 282.76]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_014_fon1.set</td>\n",
       "      <td>430.800</td>\n",
       "      <td>[2.422, 114.116, 223.771, 283.804, 330.991, 37...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "      <td>[2.422, 114.116, 223.771, 283.804, 330.991, 37...</td>\n",
       "      <td>[111.69, 109.65, 60.03, 47.19, 39.23, 228.81]</td>\n",
       "      <td>False</td>\n",
       "      <td>[47.19, 39.23, 228.81]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_016st_fon1...</td>\n",
       "      <td>724.268</td>\n",
       "      <td>[7.869, 127.416, 247.423, 367.446, 485.62, 605...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "      <td>[7.869, 127.416, 247.423, 367.446, 485.62, 605...</td>\n",
       "      <td>[119.55, 120.01, 120.02, 118.17, 120.02, -6.61]</td>\n",
       "      <td>False</td>\n",
       "      <td>[-6.61]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_020_Fon1.set</td>\n",
       "      <td>658.736</td>\n",
       "      <td>[2.762, 115.947, 222.669, 335.911, 436.163, 55...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "      <td>[2.762, 115.947, 222.669, 335.911, 436.163, 55...</td>\n",
       "      <td>[113.19, 106.72, 113.24, 100.25, 118.83, 44.04]</td>\n",
       "      <td>False</td>\n",
       "      <td>[44.04]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_021_fon1.set</td>\n",
       "      <td>727.936</td>\n",
       "      <td>[36.808, 150.913, 265.106, 382.931, 498.136, 6...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "      <td>[36.808, 150.913, 265.106, 382.931, 498.136, 6...</td>\n",
       "      <td>[114.11, 114.19, 117.82, 115.21, 119.92, -19.02]</td>\n",
       "      <td>False</td>\n",
       "      <td>[-19.02]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_022_fon1.set</td>\n",
       "      <td>680.956</td>\n",
       "      <td>[5.644, 118.733, 225.986, 342.245, 451.938, 57...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "      <td>[5.644, 118.733, 225.986, 342.245, 451.938, 57...</td>\n",
       "      <td>[113.09, 107.25, 116.26, 109.69, 118.84, 28.26]</td>\n",
       "      <td>False</td>\n",
       "      <td>[28.26]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_024_fon1.set</td>\n",
       "      <td>655.990</td>\n",
       "      <td>[8.767, 122.223, 234.441, 350.376, 448.842, 56...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "      <td>[8.767, 122.223, 234.441, 350.376, 448.842, 56...</td>\n",
       "      <td>[113.46, 112.22, 115.93, 98.47, 113.59, 36.6]</td>\n",
       "      <td>False</td>\n",
       "      <td>[36.6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_025_fon1.set</td>\n",
       "      <td>669.854</td>\n",
       "      <td>[4.929, 113.431, 223.167, 332.783, 444.887, 55...</td>\n",
       "      <td>[S   1, S   2, S   1, S   2, S   1, S   2]</td>\n",
       "      <td>[4.929, 113.431, 223.167, 332.783, 444.887, 55...</td>\n",
       "      <td>[108.5, 109.74, 109.62, 112.1, 112.34, 41.81]</td>\n",
       "      <td>False</td>\n",
       "      <td>[41.81]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_026_fon1.set</td>\n",
       "      <td>754.796</td>\n",
       "      <td>[0.0, 0.166, 48.114, 165.304, 282.819, 400.206...</td>\n",
       "      <td>[boundary, Record, S   1, S   2, S   1, S   2,...</td>\n",
       "      <td>[0.0, 0.166, 48.114, 165.304, 282.819, 400.206...</td>\n",
       "      <td>[0.17, 47.95, 117.19, 117.52, 117.39, 117.58, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.17, 47.95, -155.61]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 path  duration_sec  \\\n",
       "1    /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_008_fon1.set       718.248   \n",
       "4    /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_013_Fon1.set       366.824   \n",
       "5    /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_014_fon1.set       430.800   \n",
       "6   /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_016st_fon1...       724.268   \n",
       "7    /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_020_Fon1.set       658.736   \n",
       "8    /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_021_fon1.set       727.936   \n",
       "9    /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_022_fon1.set       680.956   \n",
       "10   /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_024_fon1.set       655.990   \n",
       "11   /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_025_fon1.set       669.854   \n",
       "12   /mnt/d/Study/PhD/Data/EEG/fon/Co_y6_026_fon1.set       754.796   \n",
       "\n",
       "                                            ann_onset  \\\n",
       "1   [8.361, 127.894, 244.88, 360.293, 478.137, 598...   \n",
       "4   [2.02, 98.647, 143.365, 205.839, 244.698, 316....   \n",
       "5   [2.422, 114.116, 223.771, 283.804, 330.991, 37...   \n",
       "6   [7.869, 127.416, 247.423, 367.446, 485.62, 605...   \n",
       "7   [2.762, 115.947, 222.669, 335.911, 436.163, 55...   \n",
       "8   [36.808, 150.913, 265.106, 382.931, 498.136, 6...   \n",
       "9   [5.644, 118.733, 225.986, 342.245, 451.938, 57...   \n",
       "10  [8.767, 122.223, 234.441, 350.376, 448.842, 56...   \n",
       "11  [4.929, 113.431, 223.167, 332.783, 444.887, 55...   \n",
       "12  [0.0, 0.166, 48.114, 165.304, 282.819, 400.206...   \n",
       "\n",
       "                                      ann_description  \\\n",
       "1          [S   1, S   2, S   1, S   2, S   1, S   2]   \n",
       "4          [S   1, S   2, S   1, S   2, S   1, S   2]   \n",
       "5          [S   1, S   2, S   1, S   2, S   1, S   2]   \n",
       "6          [S   1, S   2, S   1, S   2, S   1, S   2]   \n",
       "7          [S   1, S   2, S   1, S   2, S   1, S   2]   \n",
       "8          [S   1, S   2, S   1, S   2, S   1, S   2]   \n",
       "9          [S   1, S   2, S   1, S   2, S   1, S   2]   \n",
       "10         [S   1, S   2, S   1, S   2, S   1, S   2]   \n",
       "11         [S   1, S   2, S   1, S   2, S   1, S   2]   \n",
       "12  [boundary, Record, S   1, S   2, S   1, S   2,...   \n",
       "\n",
       "                                         all_segments  \\\n",
       "1   [8.361, 127.894, 244.88, 360.293, 478.137, 598...   \n",
       "4   [2.02, 98.647, 143.365, 205.839, 244.698, 316....   \n",
       "5   [2.422, 114.116, 223.771, 283.804, 330.991, 37...   \n",
       "6   [7.869, 127.416, 247.423, 367.446, 485.62, 605...   \n",
       "7   [2.762, 115.947, 222.669, 335.911, 436.163, 55...   \n",
       "8   [36.808, 150.913, 265.106, 382.931, 498.136, 6...   \n",
       "9   [5.644, 118.733, 225.986, 342.245, 451.938, 57...   \n",
       "10  [8.767, 122.223, 234.441, 350.376, 448.842, 56...   \n",
       "11  [4.929, 113.431, 223.167, 332.783, 444.887, 55...   \n",
       "12  [0.0, 0.166, 48.114, 165.304, 282.819, 400.206...   \n",
       "\n",
       "                                    segment_durations  good_eeg  \\\n",
       "1      [119.53, 116.99, 115.41, 117.84, 120.02, 0.87]     False   \n",
       "4         [96.63, 44.72, 62.47, 38.86, 71.58, 282.76]     False   \n",
       "5       [111.69, 109.65, 60.03, 47.19, 39.23, 228.81]     False   \n",
       "6     [119.55, 120.01, 120.02, 118.17, 120.02, -6.61]     False   \n",
       "7     [113.19, 106.72, 113.24, 100.25, 118.83, 44.04]     False   \n",
       "8    [114.11, 114.19, 117.82, 115.21, 119.92, -19.02]     False   \n",
       "9     [113.09, 107.25, 116.26, 109.69, 118.84, 28.26]     False   \n",
       "10      [113.46, 112.22, 115.93, 98.47, 113.59, 36.6]     False   \n",
       "11      [108.5, 109.74, 109.62, 112.1, 112.34, 41.81]     False   \n",
       "12  [0.17, 47.95, 117.19, 117.52, 117.39, 117.58, ...     False   \n",
       "\n",
       "              bad_segments  \n",
       "1                   [0.87]  \n",
       "4   [44.72, 38.86, 282.76]  \n",
       "5   [47.19, 39.23, 228.81]  \n",
       "6                  [-6.61]  \n",
       "7                  [44.04]  \n",
       "8                 [-19.02]  \n",
       "9                  [28.26]  \n",
       "10                  [36.6]  \n",
       "11                 [41.81]  \n",
       "12  [0.17, 47.95, -155.61]  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ann_info[~df_ann_info.good_eeg].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6ea3164",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ann_info.to_csv('df_ann_info.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463343f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e09ec21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3198/3026961692.py:8: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw = mne.io.read_raw_eeglab(eeg_path, preload=True, verbose=False)\n",
      "/tmp/ipykernel_3198/3026961692.py:22: UserWarning: nperseg=2000 is greater than signal length max(len(x), len(y)) = 166, using nperseg = 166\n",
      "  f, Pxx = welch(ch_data, fs=sfreq, nperseg=sfreq*2)\n",
      "/tmp/ipykernel_3198/3026961692.py:24: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  alpha_power = np.trapz(Pxx[(f >= 8) & (f <= 13)], f[(f >= 8) & (f <= 13)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статистика и по предположениям о состоянии глаз по каждому сегменту:\n",
      "{'segment': 8, 'desc': np.str_('Stop'), 'start_sec': 754.643, 'stop_sec': 599.032, 'duration_sec': -155.61, 'mean': None, 'std': None, 'min': None, 'max': None, 'alpha_power': None, 'inferred_eye_state': None, 'warning': 'zero-size segment'}\n",
      "{'segment': 0, 'desc': np.str_('boundary'), 'start_sec': 0.0, 'stop_sec': 0.166, 'duration_sec': 0.17, 'mean': 2.5782799632545592e-14, 'std': 5.953366703999823e-06, 'min': -3.135120964050293e-05, 'max': 3.518932723999023e-05, 'alpha_power': 0.0, 'inferred_eye_state': 'скорее всего ОТКРЫТЫ глаза (альфа↓)'}\n",
      "{'segment': 1, 'desc': np.str_('Record'), 'start_sec': 0.166, 'stop_sec': 48.114, 'duration_sec': 47.95, 'mean': -1.5106221567665995e-15, 'std': 1.140825848518049e-05, 'min': -0.0007663497924804687, 'max': 0.00323926611328125, 'alpha_power': 8.683946859338694e-12, 'inferred_eye_state': 'скорее всего ОТКРЫТЫ глаза (альфа↓)'}\n",
      "{'segment': 2, 'desc': np.str_('S   1'), 'start_sec': 48.114, 'stop_sec': 165.304, 'duration_sec': 117.19, 'mean': 1.0676188584037835e-15, 'std': 7.810198342902996e-06, 'min': -0.0001717032470703125, 'max': 0.0001567550354003906, 'alpha_power': 2.6193262560893432e-11, 'inferred_eye_state': 'скорее всего ЗАКРЫТЫ глаза (альфа↑)'}\n",
      "{'segment': 3, 'desc': np.str_('S   2'), 'start_sec': 165.304, 'stop_sec': 282.819, 'duration_sec': 117.52, 'mean': 2.092110011858541e-15, 'std': 1.0744407160262283e-05, 'min': -0.00017568168640136718, 'max': 0.0002480580596923828, 'alpha_power': 1.3641261528849596e-11, 'inferred_eye_state': 'скорее всего ОТКРЫТЫ глаза (альфа↓)'}\n",
      "{'segment': 4, 'desc': np.str_('S   1'), 'start_sec': 282.819, 'stop_sec': 400.206, 'duration_sec': 117.39, 'mean': 6.230161688351273e-16, 'std': 8.66807168286639e-06, 'min': -0.00010655516815185546, 'max': 0.00015867098999023437, 'alpha_power': 2.577242232074439e-11, 'inferred_eye_state': 'скорее всего ЗАКРЫТЫ глаза (альфа↑)'}\n",
      "{'segment': 5, 'desc': np.str_('S   2'), 'start_sec': 400.206, 'stop_sec': 517.79, 'duration_sec': 117.58, 'mean': -7.702678723928636e-16, 'std': 1.1379377376152263e-05, 'min': -0.00017755528259277343, 'max': 0.0003825446472167969, 'alpha_power': 1.3104584023739006e-11, 'inferred_eye_state': 'скорее всего ОТКРЫТЫ глаза (альфа↓)'}\n",
      "{'segment': 6, 'desc': np.str_('S   1'), 'start_sec': 517.79, 'stop_sec': 635.411, 'duration_sec': 117.62, 'mean': 2.921652690406074e-16, 'std': 1.1833347714169249e-05, 'min': -0.00011509517669677734, 'max': 0.00013752021789550781, 'alpha_power': 2.206381268302369e-11, 'inferred_eye_state': 'скорее всего ЗАКРЫТЫ глаза (альфа↑)'}\n",
      "{'segment': 7, 'desc': np.str_('S   2'), 'start_sec': 635.411, 'stop_sec': 754.643, 'duration_sec': 119.23, 'mean': 4.471211247576918e-15, 'std': 1.717838255243004e-05, 'min': -0.0012775830078124999, 'max': 0.0005618323974609375, 'alpha_power': 1.3343739208088591e-11, 'inferred_eye_state': 'скорее всего ОТКРЫТЫ глаза (альфа↓)'}\n",
      "{'segment': 8, 'desc': np.str_('Stop'), 'start_sec': 754.643, 'stop_sec': 599.032, 'duration_sec': -155.61, 'mean': None, 'std': None, 'min': None, 'max': None, 'alpha_power': None, 'inferred_eye_state': None, 'warning': 'zero-size segment'}\n"
     ]
    }
   ],
   "source": [
    "# Откроем последнее ЭЭГ-файл из df_ann_info и посчитаем статистику по сегментам\n",
    "# Определять состояние глаз будем по статистике ЭЭГ, а не по меткам S1/S2\n",
    "\n",
    "last_row = df_ann_info.iloc[12]\n",
    "eeg_path = last_row['path']\n",
    "\n",
    "# Загрузим файл с помощью mne\n",
    "raw = mne.io.read_raw_eeglab(eeg_path, preload=True, verbose=False)\n",
    "\n",
    "# Сегменты по аннотациям\n",
    "onsets = last_row['all_segments']\n",
    "descriptions = last_row['ann_description']\n",
    "segment_durations = last_row['segment_durations']\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Для примера возьмем feature: мощность альфа-ритма (8-13 Гц) как признак закрытых глаз\n",
    "def calc_alpha_power(data, sfreq):\n",
    "    from scipy.signal import welch\n",
    "    psds = []\n",
    "    for ch_data in data:\n",
    "        f, Pxx = welch(ch_data, fs=sfreq, nperseg=sfreq*2)\n",
    "        # интеграл мощности в диапазоне 8-13 Гц\n",
    "        alpha_power = np.trapz(Pxx[(f >= 8) & (f <= 13)], f[(f >= 8) & (f <= 13)])\n",
    "        psds.append(alpha_power)\n",
    "    return float(np.mean(psds))\n",
    "\n",
    "print(\"Статистика и по предположениям о состоянии глаз по каждому сегменту:\")\n",
    "\n",
    "# Для сравнения будем считать состояние глаз по относительной мощности альфа-ритма\n",
    "alpha_powers = []\n",
    "summary_stats = []\n",
    "for i in range(len(segment_durations)):\n",
    "    start = onsets[i]\n",
    "    stop = onsets[i + 1]\n",
    "    dur = segment_durations[i]\n",
    "    desc = descriptions[i] if i < len(descriptions) else None\n",
    "\n",
    "    # Получим данные сегмента (секунды в sample-индексы)\n",
    "    sfreq = raw.info['sfreq']\n",
    "    start_sample = int(start * sfreq)\n",
    "    stop_sample = int(stop * sfreq)\n",
    "    data_seg = raw.get_data()[:, start_sample:stop_sample]\n",
    "\n",
    "    if data_seg.size == 0:\n",
    "        stats = {\n",
    "            'segment': i,\n",
    "            'desc': desc,\n",
    "            'start_sec': start,\n",
    "            'stop_sec': stop,\n",
    "            'duration_sec': dur,\n",
    "            'mean': None,\n",
    "            'std': None,\n",
    "            'min': None,\n",
    "            'max': None,\n",
    "            'alpha_power': None,\n",
    "            'inferred_eye_state': None,\n",
    "            'warning': 'zero-size segment'\n",
    "        }\n",
    "        print(stats)\n",
    "        summary_stats.append(stats)\n",
    "        alpha_powers.append(0)\n",
    "        continue\n",
    "\n",
    "    mean = float(np.mean(data_seg))\n",
    "    std = float(np.std(data_seg))\n",
    "    datamin = float(np.min(data_seg))\n",
    "    datamax = float(np.max(data_seg))\n",
    "\n",
    "    alpha_power = calc_alpha_power(data_seg, sfreq)\n",
    "    alpha_powers.append(alpha_power)\n",
    "\n",
    "    stats = {\n",
    "        'segment': i,\n",
    "        'desc': desc,\n",
    "        'start_sec': start,\n",
    "        'stop_sec': stop,\n",
    "        'duration_sec': dur,\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'min': datamin,\n",
    "        'max': datamax,\n",
    "        'alpha_power': alpha_power\n",
    "    }\n",
    "    summary_stats.append(stats)\n",
    "\n",
    "# Определим threshold по альфа мощности: если мощности по сегментам bimodal, возьмём медиану как условную границу\n",
    "alpha_powers_np = np.array(alpha_powers)\n",
    "nonzero_mask = alpha_powers_np > 0\n",
    "median_alpha = np.median(alpha_powers_np[nonzero_mask]) if np.any(nonzero_mask) else 0\n",
    "\n",
    "# Оценим глазные состояния purely по статистике альфа-ритма\n",
    "for i, stats in enumerate(summary_stats):\n",
    "    alpha_power = stats.get('alpha_power')\n",
    "    if alpha_power is None:\n",
    "        stats[\"inferred_eye_state\"] = None\n",
    "    else:\n",
    "        # Предположение: при закрытых глазах альфа выше (выше медианы)\n",
    "        # Это простое разделение, для реального случая нужен анализ распределения!\n",
    "        if alpha_power > median_alpha:\n",
    "            state = 'скорее всего ЗАКРЫТЫ глаза (альфа↑)'\n",
    "        else:\n",
    "            state = 'скорее всего ОТКРЫТЫ глаза (альфа↓)'\n",
    "        stats['inferred_eye_state'] = state\n",
    "\n",
    "    print(stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "766fb0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/mnt/d/Study/PhD/Data/EEG/own/Mor_y1_004_own_face.set',\n",
       " 'duration_sec': np.float64(746.172),\n",
       " 'ann_onset': [5.4095,\n",
       "  6.2615,\n",
       "  6.8955,\n",
       "  19.159,\n",
       "  137.659,\n",
       "  260.059,\n",
       "  378.359,\n",
       "  498.859,\n",
       "  618.359],\n",
       " 'ann_description': [np.str_('boundary'),\n",
       "  np.str_('boundary'),\n",
       "  np.str_('boundary'),\n",
       "  np.str_('S1'),\n",
       "  np.str_('S2'),\n",
       "  np.str_('S1'),\n",
       "  np.str_('S2'),\n",
       "  np.str_('S1'),\n",
       "  np.str_('S2')],\n",
       " 'all_segments': [5.4095,\n",
       "  6.2615,\n",
       "  6.8955,\n",
       "  19.159,\n",
       "  137.659,\n",
       "  260.059,\n",
       "  378.359,\n",
       "  498.859,\n",
       "  618.359,\n",
       "  599.032],\n",
       " 'segment_durations': [0.85,\n",
       "  0.63,\n",
       "  12.26,\n",
       "  118.5,\n",
       "  122.4,\n",
       "  118.3,\n",
       "  120.5,\n",
       "  119.5,\n",
       "  -19.33],\n",
       " 'good_eeg': np.False_,\n",
       " 'bad_segments': [0.85, 0.63, 12.26, -19.33]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(df_ann_info.iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87a5ae97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/mnt/d/Study/PhD/Data/EEG/fon/Co_y6_013_Fon1.set',\n",
       " 'duration_sec': np.float64(366.824),\n",
       " 'ann_onset': [2.02, 98.647, 143.365, 205.839, 244.698, 316.274],\n",
       " 'ann_description': [np.str_('S   1'),\n",
       "  np.str_('S   2'),\n",
       "  np.str_('S   1'),\n",
       "  np.str_('S   2'),\n",
       "  np.str_('S   1'),\n",
       "  np.str_('S   2')],\n",
       " 'all_segments': [2.02, 98.647, 143.365, 205.839, 244.698, 316.274, 599.032],\n",
       " 'segment_durations': [96.63, 44.72, 62.47, 38.86, 71.58, 282.76]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(df_ann_info.iloc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2b6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6f42e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa57a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading EEGs:   0%|          | 0/96 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend LokyBackend with 32 concurrent workers.\n",
      "Loading EEGs:  33%|███▎      | 32/96 [00:00<00:00, 206.85it/s]/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/pymatreader/utils.py:179: UserWarning: Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\n",
      "  warn(\n",
      "Exception ignored in: <_io.BytesIO object at 0x773d5c53df30>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/joblib/externals/loky/process_executor.py\", line 110, in _get_memory_usage\n",
      "    gc.collect()\n",
      "BufferError: Existing exports of data: object cannot be re-sized\n",
      "Loading EEGs:  33%|███▎      | 32/96 [00:19<00:00, 206.85it/s]"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import traceback\n",
    "\n",
    "def load_eeg_file(path, target_sfreq=500):\n",
    "    \"\"\"Load and resample a single EEG file, return (data, path) or (None, path) on error.\"\"\"\n",
    "    try:\n",
    "        raw = mne.io.read_raw_eeglab(path, preload=True, verbose=False)\n",
    "        if raw.info[\"sfreq\"] != target_sfreq:\n",
    "            raw.resample(target_sfreq, verbose=False)\n",
    "        data = raw.get_data()  # shape: (n_channels, n_samples)\n",
    "        return (data, path)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR loading file: {path}\")\n",
    "        print(f\"Error type: {type(e).__name__}\")\n",
    "        print(f\"Error message: {str(e)}\")\n",
    "        return (None, path)  # Return None for data, but keep path for tracking\n",
    "\n",
    "# Load all files in parallel (adjust n_jobs based on your CPU cores)\n",
    "results = Parallel(n_jobs=-1, verbose=1)(\n",
    "    delayed(load_eeg_file)(path, target_sfreq) \n",
    "    for path in tqdm(df_eegs['path'], desc=\"Loading EEGs\")\n",
    ")\n",
    "\n",
    "# Separate successful loads from errors\n",
    "array_eegs = [data for data, path in results if data is not None]\n",
    "error_paths = [path for data, path in results if data is None]\n",
    "\n",
    "print(f\"\\nSuccessfully loaded: {len(array_eegs)}/{len(results)} files\")\n",
    "if error_paths:\n",
    "    print(f\"Failed to load {len(error_paths)} files:\")\n",
    "    for path in error_paths:\n",
    "        print(f\"  - {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a891d853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(array_eegs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a964635e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1624b85e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b2e7d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "016240de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = raw.annotations\n",
    "ann.onset, ann.duration, ann.description\n",
    "\n",
    "segmen_starts = [float(seg_start) for seg_start in ann.onset] + [float(raw.duration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cfab0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = [(float(segmen_starts[i]) + segment_margin_sec, float(segmen_starts[i+1]) - segment_margin_sec) for i in range(len(segmen_starts) - 1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0db5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = [0, 1, 0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "424af742",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_crops = defaultdict(list)\n",
    "\n",
    "for event, interval in zip(events, intervals):\n",
    "    interval_segments = []\n",
    "    interval_start = interval[0]\n",
    "    interval_end = interval[1]\n",
    "\n",
    "    while interval_start < interval_end:\n",
    "        interval_segments.append((int(interval_start * target_sfreq), int((interval_start + segment_sec) * target_sfreq)))\n",
    "        interval_start += segment_sec - overlap_sec\n",
    "    eeg_crops[event].extend(interval_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04b991f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15300"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "85 * 2 * 30 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec0716ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop numpy array from raw EEG data\n",
    "eeg_array = raw.get_data()\n",
    "\n",
    "crop = eeg_array[:, eeg_crops[1][0][0]:eeg_crops[1][0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71507e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a03c08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "# Take batch_size different crops from eeg_array based on eeg_crops[1] and move to cuda\n",
    "crop_batch = np.stack([\n",
    "    eeg_array[:, eeg_crops[1][i][0]:eeg_crops[1][i][1]] \n",
    "    for i in range(batch_size)\n",
    "])\n",
    "crop_batch = torch.from_numpy(crop_batch).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdd4c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [batch_size] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5131cb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1056da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, n_channels=128, in_samples=2500, n_classes=2):\n",
    "        super(EEGNet, self).__init__()\n",
    "        # First temporal convolution\n",
    "        self.conv1 = nn.Conv1d(n_channels, 64, kernel_size=7, stride=1, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        # Depthwise convolution\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2, groups=64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        # Pointwise convolution\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=1)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        # Temporal pooling\n",
    "        self.pool = nn.AdaptiveAvgPool1d(32)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        # Fully connected classification head\n",
    "        self.fc1 = nn.Linear(256 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, n_channels, in_samples)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x  # logits\n",
    "\n",
    "# Usage example for (batch_size, 128, 2500) input\n",
    "eeg_sample = torch.randn(batch_size, 128, 2500).to('cuda')\n",
    "model = EEGNet().to('cuda')\n",
    "out = model(eeg_sample)  # out shape: (batch_size, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "085cbc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "# class EEGTransformer(nn.Module):\n",
    "#     def __init__(self, n_channels=128, in_samples=2500, n_classes=2, d_model=128, nhead=8, num_layers=4, dim_feedforward=256, dropout=0.3):\n",
    "#         super(EEGTransformer, self).__init__()\n",
    "#         self.pos_embedding = nn.Parameter(torch.zeros(1, n_channels, d_model))\n",
    "#         self.input_proj = nn.Linear(in_samples, d_model)\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(\n",
    "#             d_model=d_model,\n",
    "#             nhead=nhead,\n",
    "#             dim_feedforward=dim_feedforward,\n",
    "#             dropout=dropout,\n",
    "#             activation='gelu',\n",
    "#             batch_first=True)\n",
    "#         self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.LayerNorm(d_model),\n",
    "#             nn.Linear(d_model, n_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: (batch_size, n_channels, in_samples)\n",
    "#         x = self.input_proj(x)                     # (batch_size, n_channels, d_model)\n",
    "#         x = x + self.pos_embedding                 # positional encoding\n",
    "#         out = self.transformer_encoder(x)          # (batch_size, n_channels, d_model)\n",
    "#         out = out.mean(dim=1)                      # (batch_size, d_model) - global avg pooling channels\n",
    "#         out = self.classifier(out)                 # (batch_size, n_classes)\n",
    "#         return out  # logits\n",
    "\n",
    "# # Usage example for (batch_size, 128, 2500) input\n",
    "# eeg_sample = torch.randn(batch_size, 128, 2500).to('cuda')\n",
    "# transf_model = EEGTransformer(n_channels=128, in_samples=2500, n_classes=2).to('cuda')\n",
    "# out_transformer = transf_model(eeg_sample)  # out shape: (batch_size, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7850006a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a740d37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whatislove/miniconda3/envs/phd/lib/python3.13/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "421ba7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f7b1faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_crop = torch.from_numpy(crop).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b182d3ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3564e-06,  1.0416e-06,  1.9863e-07,  ..., -2.0719e-06,\n",
       "         -2.5011e-06, -3.1162e-06],\n",
       "        [-5.7126e-06, -5.4713e-06, -5.1856e-06,  ..., -1.4616e-06,\n",
       "         -1.9963e-06, -2.9750e-06],\n",
       "        [ 3.6175e-06,  3.5242e-06,  3.2241e-06,  ..., -2.1745e-06,\n",
       "         -2.2885e-06, -2.6498e-06],\n",
       "        ...,\n",
       "        [ 4.5774e-06,  5.0745e-06,  5.7608e-06,  ...,  3.7391e-07,\n",
       "          2.8602e-07,  5.0206e-07],\n",
       "        [ 5.8502e-06,  6.0943e-06,  6.1615e-06,  ...,  8.5529e-07,\n",
       "          7.4678e-07,  1.1209e-06],\n",
       "        [-2.0557e-07,  6.2699e-07,  1.3594e-06,  ...,  4.2139e-07,\n",
       "          3.4264e-07,  7.3738e-07]], device='cuda:0')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_crop.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "122d68f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "batch = torch.stack([torch_crop] * 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0675a801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3564e-06,  1.0416e-06,  1.9863e-07,  ..., -2.0719e-06,\n",
       "          -2.5011e-06, -3.1162e-06],\n",
       "         [-5.7126e-06, -5.4713e-06, -5.1856e-06,  ..., -1.4616e-06,\n",
       "          -1.9963e-06, -2.9750e-06],\n",
       "         [ 3.6175e-06,  3.5242e-06,  3.2241e-06,  ..., -2.1745e-06,\n",
       "          -2.2885e-06, -2.6498e-06],\n",
       "         ...,\n",
       "         [ 4.5774e-06,  5.0745e-06,  5.7608e-06,  ...,  3.7391e-07,\n",
       "           2.8602e-07,  5.0206e-07],\n",
       "         [ 5.8502e-06,  6.0943e-06,  6.1615e-06,  ...,  8.5529e-07,\n",
       "           7.4678e-07,  1.1209e-06],\n",
       "         [-2.0557e-07,  6.2699e-07,  1.3594e-06,  ...,  4.2139e-07,\n",
       "           3.4264e-07,  7.3738e-07]],\n",
       "\n",
       "        [[ 1.3564e-06,  1.0416e-06,  1.9863e-07,  ..., -2.0719e-06,\n",
       "          -2.5011e-06, -3.1162e-06],\n",
       "         [-5.7126e-06, -5.4713e-06, -5.1856e-06,  ..., -1.4616e-06,\n",
       "          -1.9963e-06, -2.9750e-06],\n",
       "         [ 3.6175e-06,  3.5242e-06,  3.2241e-06,  ..., -2.1745e-06,\n",
       "          -2.2885e-06, -2.6498e-06],\n",
       "         ...,\n",
       "         [ 4.5774e-06,  5.0745e-06,  5.7608e-06,  ...,  3.7391e-07,\n",
       "           2.8602e-07,  5.0206e-07],\n",
       "         [ 5.8502e-06,  6.0943e-06,  6.1615e-06,  ...,  8.5529e-07,\n",
       "           7.4678e-07,  1.1209e-06],\n",
       "         [-2.0557e-07,  6.2699e-07,  1.3594e-06,  ...,  4.2139e-07,\n",
       "           3.4264e-07,  7.3738e-07]],\n",
       "\n",
       "        [[ 1.3564e-06,  1.0416e-06,  1.9863e-07,  ..., -2.0719e-06,\n",
       "          -2.5011e-06, -3.1162e-06],\n",
       "         [-5.7126e-06, -5.4713e-06, -5.1856e-06,  ..., -1.4616e-06,\n",
       "          -1.9963e-06, -2.9750e-06],\n",
       "         [ 3.6175e-06,  3.5242e-06,  3.2241e-06,  ..., -2.1745e-06,\n",
       "          -2.2885e-06, -2.6498e-06],\n",
       "         ...,\n",
       "         [ 4.5774e-06,  5.0745e-06,  5.7608e-06,  ...,  3.7391e-07,\n",
       "           2.8602e-07,  5.0206e-07],\n",
       "         [ 5.8502e-06,  6.0943e-06,  6.1615e-06,  ...,  8.5529e-07,\n",
       "           7.4678e-07,  1.1209e-06],\n",
       "         [-2.0557e-07,  6.2699e-07,  1.3594e-06,  ...,  4.2139e-07,\n",
       "           3.4264e-07,  7.3738e-07]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.3564e-06,  1.0416e-06,  1.9863e-07,  ..., -2.0719e-06,\n",
       "          -2.5011e-06, -3.1162e-06],\n",
       "         [-5.7126e-06, -5.4713e-06, -5.1856e-06,  ..., -1.4616e-06,\n",
       "          -1.9963e-06, -2.9750e-06],\n",
       "         [ 3.6175e-06,  3.5242e-06,  3.2241e-06,  ..., -2.1745e-06,\n",
       "          -2.2885e-06, -2.6498e-06],\n",
       "         ...,\n",
       "         [ 4.5774e-06,  5.0745e-06,  5.7608e-06,  ...,  3.7391e-07,\n",
       "           2.8602e-07,  5.0206e-07],\n",
       "         [ 5.8502e-06,  6.0943e-06,  6.1615e-06,  ...,  8.5529e-07,\n",
       "           7.4678e-07,  1.1209e-06],\n",
       "         [-2.0557e-07,  6.2699e-07,  1.3594e-06,  ...,  4.2139e-07,\n",
       "           3.4264e-07,  7.3738e-07]],\n",
       "\n",
       "        [[ 1.3564e-06,  1.0416e-06,  1.9863e-07,  ..., -2.0719e-06,\n",
       "          -2.5011e-06, -3.1162e-06],\n",
       "         [-5.7126e-06, -5.4713e-06, -5.1856e-06,  ..., -1.4616e-06,\n",
       "          -1.9963e-06, -2.9750e-06],\n",
       "         [ 3.6175e-06,  3.5242e-06,  3.2241e-06,  ..., -2.1745e-06,\n",
       "          -2.2885e-06, -2.6498e-06],\n",
       "         ...,\n",
       "         [ 4.5774e-06,  5.0745e-06,  5.7608e-06,  ...,  3.7391e-07,\n",
       "           2.8602e-07,  5.0206e-07],\n",
       "         [ 5.8502e-06,  6.0943e-06,  6.1615e-06,  ...,  8.5529e-07,\n",
       "           7.4678e-07,  1.1209e-06],\n",
       "         [-2.0557e-07,  6.2699e-07,  1.3594e-06,  ...,  4.2139e-07,\n",
       "           3.4264e-07,  7.3738e-07]],\n",
       "\n",
       "        [[ 1.3564e-06,  1.0416e-06,  1.9863e-07,  ..., -2.0719e-06,\n",
       "          -2.5011e-06, -3.1162e-06],\n",
       "         [-5.7126e-06, -5.4713e-06, -5.1856e-06,  ..., -1.4616e-06,\n",
       "          -1.9963e-06, -2.9750e-06],\n",
       "         [ 3.6175e-06,  3.5242e-06,  3.2241e-06,  ..., -2.1745e-06,\n",
       "          -2.2885e-06, -2.6498e-06],\n",
       "         ...,\n",
       "         [ 4.5774e-06,  5.0745e-06,  5.7608e-06,  ...,  3.7391e-07,\n",
       "           2.8602e-07,  5.0206e-07],\n",
       "         [ 5.8502e-06,  6.0943e-06,  6.1615e-06,  ...,  8.5529e-07,\n",
       "           7.4678e-07,  1.1209e-06],\n",
       "         [-2.0557e-07,  6.2699e-07,  1.3594e-06,  ...,  4.2139e-07,\n",
       "           3.4264e-07,  7.3738e-07]]], device='cuda:0')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6eb007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty memory cuda\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1474fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tensor from cuda\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b9d6eb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599.032"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "299516 / 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68599ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "1736 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774d9cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "00c00065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(295803, 298303)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_crops[1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4670ae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2737c72b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ann' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mann\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'ann' is not defined"
     ]
    }
   ],
   "source": [
    "ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6918d83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of annotations: 6\n",
      "41.292 31.293\n",
      "137.311 127.31200000000001\n",
      "223.231 213.232\n",
      "310.396 300.397\n",
      "404.016 394.017\n",
      "501.607 491.608\n",
      "Number of clean intervals: 0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BUILD CLEAN EVENT INTERVALS (IN SAMPLES)\n",
    "# ============================================================\n",
    "\n",
    "intervals = []  # list of (start_sample, end_sample, label)\n",
    "\n",
    "ann = raw.annotations\n",
    "print(\"Number of annotations:\", len(ann))\n",
    "\n",
    "for onset, duration, desc in zip(ann.onset, ann.duration, ann.description):\n",
    "    if desc not in label_map:\n",
    "        continue  # ignore annotations we don't care about\n",
    "\n",
    "    # Convert onset + margin to sample index using raw.time_as_index()\n",
    "    # This handles any offset, meas_date, etc.\n",
    "    clean_start_time = onset + L_sec\n",
    "    clean_end_time   = onset - L_sec\n",
    "    print(clean_start_time, clean_end_time)\n",
    "    # If event is too short for the margin, skip it\n",
    "    if clean_end_time <= clean_start_time:\n",
    "        continue\n",
    "\n",
    "    start_sample = raw.time_as_index(clean_start_time)[0]\n",
    "    end_sample   = raw.time_as_index(clean_end_time)[0]\n",
    "\n",
    "    # Clip to recording bounds\n",
    "    start_sample = max(start_sample, 0)\n",
    "    end_sample   = min(end_sample, n_samples - 1)\n",
    "\n",
    "    if end_sample <= start_sample:\n",
    "        continue\n",
    "\n",
    "    label = label_map[desc]\n",
    "    intervals.append((start_sample, end_sample, label))\n",
    "\n",
    "print(\"Number of clean intervals:\", len(intervals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb3df15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe125d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3306389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled sfreq: 500.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# OPTIONAL: sort by start_sample\n",
    "intervals.sort(key=lambda x: x[0])\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SLIDING WINDOWS OVER FULL RECORDING\n",
    "# ============================================================\n",
    "\n",
    "window_len_samples = int(round(N_sec * sfreq))\n",
    "step_samples       = int(round(step_sec * sfreq))\n",
    "\n",
    "print(\"window_len_samples:\", window_len_samples)\n",
    "print(\"step_samples:\", step_samples)\n",
    "\n",
    "start_indices = np.arange(0, n_samples - window_len_samples + 1, step_samples)\n",
    "n_windows = len(start_indices)\n",
    "print(\"Number of windows:\", n_windows)\n",
    "\n",
    "# Allocate arrays\n",
    "X = np.empty((n_windows, n_channels, window_len_samples), dtype=np.float32)\n",
    "y = np.zeros(n_windows, dtype=np.int64)  # 0 = background by default\n",
    "\n",
    "def assign_label_for_window(w_start, w_end, intervals):\n",
    "    \"\"\"\n",
    "    Return label for window [w_start, w_end) in samples.\n",
    "    Rule:\n",
    "      - If window is fully inside exactly one clean interval: return that label\n",
    "      - If window is inside multiple intervals: use the first one\n",
    "      - If window doesn't fit any clean interval: return 0 (background)\n",
    "    \"\"\"\n",
    "    for (s, e, lab) in intervals:\n",
    "        if w_start >= s and w_end <= e:\n",
    "            return lab\n",
    "    return 0\n",
    "\n",
    "for i, w_start in enumerate(start_indices):\n",
    "    w_end = w_start + window_len_samples\n",
    "\n",
    "    # Extract data segment\n",
    "    X[i] = data[:, w_start:w_end]\n",
    "\n",
    "    # Assign label based on clean intervals\n",
    "    y[i] = assign_label_for_window(w_start, w_end, intervals)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y unique labels:\", np.unique(y))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# SAVE RESULT\n",
    "# ============================================================\n",
    "\n",
    "# Optional: time axis for each window (relative, seconds)\n",
    "time_axis = np.arange(window_len_samples) / sfreq\n",
    "\n",
    "np.savez(\n",
    "    out_file,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    sfreq=sfreq,\n",
    "    ch_names=np.array(raw.ch_names),\n",
    "    window_len_sec=N_sec,\n",
    "    overlap_sec=M_sec,\n",
    "    margin_sec=L_sec,\n",
    "    time_axis=time_axis,\n",
    ")\n",
    "\n",
    "print(\"Saved to:\", out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "324e6fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: [np.str_('S   1'), np.str_('S   2')]\n",
      "Found event codes: {np.str_('S   1'): 1, np.str_('S   2'): 2}\n"
     ]
    }
   ],
   "source": [
    "# 3) Extract events from annotations\n",
    "#    For EEGLAB imports, MNE usually stores markers as annotations.\n",
    "events, mne_event_id = mne.events_from_annotations(raw)\n",
    "print(\"Found event codes:\", mne_event_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9eabbf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using event_id: {np.str_('S   1'): 1, np.str_('S   2'): 2}\n",
      "Not setting metadata\n",
      "6 matching events found\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 6 events and 501 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    }
   ],
   "source": [
    "print(\"Using event_id:\", event_id)\n",
    "\n",
    "# 4) Create epochs (crops around each event)\n",
    "epochs = mne.Epochs(\n",
    "    raw,\n",
    "    events=events,\n",
    "    event_id=event_id,\n",
    "    tmin=tmin,\n",
    "    tmax=tmax,\n",
    "    baseline=baseline,\n",
    "    preload=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a10891b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs data shape: (6, 128, 501)\n"
     ]
    }
   ],
   "source": [
    "print(\"Epochs data shape:\", epochs.get_data().shape)  # (n_epochs, n_channels, n_times)\n",
    "\n",
    "# 5) Extract NN-ready arrays\n",
    "# X: (n_epochs, n_channels, n_times)\n",
    "X = epochs.get_data().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edb9377f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 501)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a92d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_raw: event code for each epoch (integers from event_id)\n",
    "y_raw = epochs.events[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f42d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map event codes to 0..K-1 (more convenient for NN)\n",
    "unique_codes = np.sort(np.unique(y_raw))\n",
    "code_to_idx = {code: idx for idx, code in enumerate(unique_codes)}\n",
    "y = np.array([code_to_idx[c] for c in y_raw], dtype=np.int64)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"Unique labels (0..K-1):\", np.unique(y))\n",
    "\n",
    "# 6) Save for later use in your NN\n",
    "np.savez(\n",
    "    out_file,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    sfreq=epochs.info[\"sfreq\"],\n",
    "    ch_names=np.array(epochs.ch_names),\n",
    "    tmin=tmin,\n",
    "    tmax=tmax,\n",
    ")\n",
    "print(\"Saved:\", out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28294175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
