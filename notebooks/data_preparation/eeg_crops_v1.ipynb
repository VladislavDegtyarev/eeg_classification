{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1112e843",
   "metadata": {},
   "source": [
    "# Нарезка кропов для обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966d0613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad74186",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_parquet('../../data/classification/raw/segments_split_v1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49108ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# known warning suppression\n",
    "\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Suppress warnings from Python, numpy, and C-level (including joblib workers)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "if hasattr(sys, 'warnoptions'):\n",
    "    sys.warnoptions = []\n",
    "\n",
    "def block_warnings_in_worker():\n",
    "    import warnings, os\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "# Patch: run warning suppression inside the function/context run by joblib\n",
    "from functools import wraps\n",
    "\n",
    "def suppress_worker_warnings(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        block_warnings_in_worker()\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef2a5da",
   "metadata": {},
   "source": [
    "## Нарезка и сохранение кропов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ef64f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EEGCropsConfig:\n",
    "    # Preprocessing params\n",
    "    target_sfreq: int = 500  # Hz\n",
    "    notch_filter: list[int] = field(default_factory=lambda: [50, 60])\n",
    "    bandpass_filter: list[float] = field(default_factory=lambda: [0.5, 100])\n",
    "\n",
    "    # Crop params\n",
    "    segment_sec: float = 5.0                # window length in seconds (crop length)\n",
    "    overlap_sec: float = 0.0                # overlap in seconds\n",
    "    segment_margin_sec: float = 5.0         # clean margin from event start/end in seconds\n",
    "\n",
    "# channels\n",
    "EEG_USEFUL_30: list[str] = [\n",
    "    'Fp1', 'Fp2',\n",
    "    'AF3', 'AF4',\n",
    "    'F7', 'F3', 'Fz', 'F4', 'F8',\n",
    "    'FC1', 'FCz', 'FC2',\n",
    "    'T7', 'C3', 'C4', 'T8',\n",
    "    'CP1', 'CPz', 'CP2',\n",
    "    'P7', 'P3', 'Pz', 'P4', 'P8',\n",
    "    'PO3', 'POz', 'PO4',\n",
    "    'O1', 'Oz', 'O2',\n",
    "]\n",
    "EEG_USEFUL_62: list[str] = [\n",
    "    'Fp1', 'Fp2',\n",
    "    'AF3', 'AF4',\n",
    "    'F7', 'F3', 'Fz', 'F4', 'F8',\n",
    "    'FC1', 'FCz', 'FC2',\n",
    "    'T7', 'C3', 'C4', 'T8',\n",
    "    'CP1', 'CPz', 'CP2',\n",
    "    'P7', 'P3', 'Pz', 'P4', 'P8',\n",
    "    'PO3', 'POz', 'PO4',\n",
    "    'O1', 'Oz', 'O2',\n",
    "    # extra frontal / prefrontal\n",
    "    'AF7', 'AF8', 'AFp1', 'AFp2',\n",
    "    'F1', 'F2', 'F5', 'F6',\n",
    "    # extra fronto-central (NOTE: FC5/FC6 removed to make total = 64)\n",
    "    'FT7', 'FT8', 'FC3', 'FC4',\n",
    "    # extra temporal / temporo-parietal\n",
    "    'FT9', 'FT10', 'TP7', 'TP8', 'TP9', 'TP10',\n",
    "    # extra central\n",
    "    'C1', 'C2', 'C5', 'C6',\n",
    "    # extra centro-parietal\n",
    "    'CP3', 'CP4', 'CP5', 'CP6',\n",
    "    # extra parietal + parieto-occipital\n",
    "    'P1', 'P2', 'P5', 'P6', 'PO7', 'PO8',\n",
    "]\n",
    "\n",
    "\n",
    "crop_config = EEGCropsConfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b1fa67a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from functools import wraps\n",
    "from collections import Counter\n",
    "\n",
    "def ignore_mne_warnings(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\"The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\", category=RuntimeWarning)\n",
    "            warnings.filterwarnings(\"ignore\", message=\"Complex objects (like classes) are not supported. They are imported on a best effort base but your mileage will vary.\", category=UserWarning)\n",
    "            return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "def process_full_segments(raw, crop_config, segment_list, good_mask, save_info):\n",
    "    crops = []\n",
    "    target_sfreq = crop_config.target_sfreq\n",
    "    segment_sec = crop_config.segment_sec\n",
    "    required_samples = int(round(target_sfreq * segment_sec))\n",
    "    eps = 1e-5\n",
    "\n",
    "    for i, (event, is_good) in enumerate(zip(segment_list, good_mask)):\n",
    "        start, end = event\n",
    "        crop_start = start + crop_config.segment_margin_sec\n",
    "        crop_end = end - crop_config.segment_margin_sec\n",
    "        t = crop_start\n",
    "        while t + segment_sec <= crop_end + eps:\n",
    "            # Compute sample indices to get exactly required_samples\n",
    "            start_sample = int(round(t * target_sfreq))\n",
    "            stop_sample = start_sample + required_samples\n",
    "\n",
    "            crop = raw.get_data(start=start_sample, stop=stop_sample)\n",
    "\n",
    "            # Double-check crop shape\n",
    "            if crop.shape[1] != required_samples:\n",
    "                print(f\"WARNING: Crop at t={t:.3f} has shape[1]={crop.shape[1]} (should be {required_samples}). Adjusting...\")\n",
    "\n",
    "                if crop.shape[1] > required_samples:\n",
    "                    crop = crop[:, :required_samples]\n",
    "                else:  # pad with zeros if too short (rare)\n",
    "                    pad_shape = (crop.shape[0], required_samples - crop.shape[1])\n",
    "                    crop = np.concatenate([crop, np.zeros(pad_shape, dtype=crop.dtype)], axis=1)\n",
    "\n",
    "            crop_info = {\n",
    "                # инфа о кропе\n",
    "                \"crop_start_time\": t,\n",
    "                \"crop_end_time\": t + segment_sec,\n",
    "                \"event_type\": save_info.get('event_type', 'evt'),\n",
    "            }\n",
    "\n",
    "            # Correctly source info from save_info, with None check (to avoid NoneType errors)\n",
    "            event_type = save_info.get('event_type', 'evt') if save_info else 'evt'\n",
    "            subject_id = save_info.get('subject_id') if save_info else None\n",
    "            session_type = save_info.get('session_type') if save_info else None\n",
    "            path = save_info.get('path') if save_info else None\n",
    "            crops_dir = save_info.get('crops_dir') if save_info else None\n",
    "\n",
    "            # Check that path and crops_dir are not None\n",
    "            if path is None or crops_dir is None:\n",
    "                print(f\"Skipping crop {i}: missing 'path' or 'crops_dir' (path={path}, crops_dir={crops_dir})\")\n",
    "                t += segment_sec - crop_config.overlap_sec\n",
    "                continue\n",
    "\n",
    "            base_fn = os.path.splitext(os.path.basename(path))[0]\n",
    "            crop_id = f\"{base_fn}_{session_type}_{event_type}_{i}_{int(1000*t)}_{int(1000*(t+segment_sec))}.npy\"\n",
    "            crop_path = os.path.join(crops_dir, crop_id)\n",
    "            np.save(crop_path, crop.astype(np.float32))\n",
    "\n",
    "            crop_info['crop_path'] = crop_path\n",
    "            crop_info['path'] = path  # добавляем исходный путь до ээг\n",
    "\n",
    "            crops.append(crop_info)\n",
    "\n",
    "            t += segment_sec - crop_config.overlap_sec\n",
    "    return crops\n",
    "\n",
    "@ignore_mne_warnings\n",
    "def crop_eeg_full_events(\n",
    "    df_segments: pd.DataFrame,\n",
    "    crop_config,\n",
    "    channels_to_keep: list[str],\n",
    "    crops_dir: str\n",
    ") -> pd.DataFrame:\n",
    "    os.makedirs(crops_dir, exist_ok=True)\n",
    "\n",
    "    target_sfreq = crop_config.target_sfreq\n",
    "    notch_filter = crop_config.notch_filter\n",
    "    bandpass_filter = crop_config.bandpass_filter\n",
    "\n",
    "    missing_channels_counter = Counter()\n",
    "    channels_stats_counter = Counter()\n",
    "    all_crop_rows = []\n",
    "\n",
    "    df_iter = df_segments.itertuples(index=False)\n",
    "    with tqdm(total=len(df_segments), desc=\"Cropping EEGs from full events\") as pbar:\n",
    "        for row in df_iter:\n",
    "            path = getattr(row, 'path', None)\n",
    "            segs_S1 = getattr(row, 'segments_S1', None)\n",
    "            segs_S2 = getattr(row, 'segments_S2', None)\n",
    "            good_S1 = getattr(row, 'good_S1', [])\n",
    "            good_S2 = getattr(row, 'good_S2', [])\n",
    "\n",
    "            # Defensive check for path\n",
    "            if path is None:\n",
    "                print(f\"Skipping row: missing EEG file path.\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            # save_info -- информация о файле и сессии, никакой инфы о сегменте\n",
    "            save_info_base = {\n",
    "                \"path\": path,\n",
    "                \"crops_dir\": crops_dir,\n",
    "                \"subject_id\": getattr(row, 'subject_id', None),\n",
    "                \"session_type\": getattr(row, 'session_type', None)\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\"ignore\", message=\"The data contains 'boundary' events, indicating data discontinuities.\", category=RuntimeWarning)\n",
    "                    warnings.filterwarnings(\"ignore\", message=\"Complex objects (like classes) are not supported.\", category=UserWarning)\n",
    "                    raw = mne.io.read_raw_eeglab(path, preload=True, verbose=False)\n",
    "                channels_present = [ch for ch in channels_to_keep if ch in raw.ch_names]\n",
    "                missing_channels = [ch for ch in channels_to_keep if ch not in channels_present]\n",
    "                key = tuple(sorted(missing_channels))\n",
    "                if key not in missing_channels_counter:\n",
    "                    missing_channels_counter[key] = 0\n",
    "                missing_channels_counter[key] += 1\n",
    "                channels_stats_counter[len(channels_present)] = channels_stats_counter.get(len(channels_present), 0) + 1\n",
    "\n",
    "                raw.pick(channels_present)\n",
    "\n",
    "                if raw.info['sfreq'] != target_sfreq:\n",
    "                    raw.resample(target_sfreq)\n",
    "\n",
    "                raw.filter(l_freq=bandpass_filter[0], h_freq=bandpass_filter[1], fir_design='firwin', verbose=False)\n",
    "\n",
    "                if isinstance(notch_filter, (list, tuple)):\n",
    "                    freqs_to_notch = notch_filter\n",
    "                else:\n",
    "                    freqs_to_notch = [notch_filter]\n",
    "                nyquist = raw.info['sfreq'] / 2.\n",
    "                used_freqs = [f for f in freqs_to_notch if f < nyquist]\n",
    "                if used_freqs:\n",
    "                    raw.notch_filter(freqs=used_freqs, fir_design='firwin', verbose=False)\n",
    "\n",
    "                # Handle S1 segments\n",
    "                if segs_S1 is not None and len(segs_S1) > 0:\n",
    "                    save_info_S1 = dict(save_info_base)\n",
    "                    save_info_S1['event_type'] = 'S1'\n",
    "                    crop_rows_S1 = process_full_segments(\n",
    "                        raw, crop_config, segs_S1, good_S1, save_info_S1\n",
    "                    )\n",
    "                    all_crop_rows.extend(crop_rows_S1)\n",
    "\n",
    "                # Handle S2 segments\n",
    "                if segs_S2 is not None and len(segs_S2) > 0:\n",
    "                    save_info_S2 = dict(save_info_base)\n",
    "                    save_info_S2['event_type'] = 'S2'\n",
    "                    crop_rows_S2 = process_full_segments(\n",
    "                        raw, crop_config, segs_S2, good_S2, save_info_S2\n",
    "                    )\n",
    "                    all_crop_rows.extend(crop_rows_S2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed {path}: {e}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "    df_crops = pd.DataFrame(all_crop_rows)\n",
    "\n",
    "    if df_crops.empty:\n",
    "        print(\"!!! Кропы не были сохранены или данные отсутствуют: итоговый df пустой !!!\")\n",
    "\n",
    "    print(\"=== Статистика числа каналов в crop ===\")\n",
    "    for n_ch, count in sorted(channels_stats_counter.items()):\n",
    "        print(f\"{n_ch} каналов: {count} кропов\")\n",
    "\n",
    "    print(\"\\n=== Топ-10 самых частых комбинаций отсутствующих каналов ===\")\n",
    "    missed_sorted = Counter(missing_channels_counter).most_common(10)\n",
    "    for miss, count in missed_sorted:\n",
    "        if len(miss) == 0:\n",
    "            continue\n",
    "        print(f\"{count} кропов без каналов: {miss}\")\n",
    "\n",
    "    return df_crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d0041842",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '/Users/whatislove/study/phd/data/processed/crops_v1_c1_500hz_30ch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5d769fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping EEGs from full events: 100%|██████████| 80/80 [02:07<00:00,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Статистика числа каналов в crop ===\n",
      "30 каналов: 80 кропов\n",
      "\n",
      "=== Топ-10 самых частых комбинаций отсутствующих каналов ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_crops = crop_eeg_full_events(df_raw, crop_config, EEG_USEFUL_30, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4cdbe100",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(df_raw, df_crops, on='path', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e3c21e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_parquet('../../data/classification/raw/crops_500_30_v1.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09d14566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_final = pd.read_parquet('../../data/classification/raw/crops_500_30_v1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0408c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Датасет для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a5674dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['path', 'crop_path', 'subject_id', 'session_type', 'crop_start_time', 'crop_end_time', 'event_type', 'good_S1', 'good_S2', 'segments_S1', 'segments_S2']\n",
    "\n",
    "df_train = df_final[columns].loc[df_final['split'] == 'train']\n",
    "df_valid = df_final[columns].loc[df_final['split'] == 'valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## метки, классификация состояния глаз S1/S2 открыты закрыты\n",
    "\n",
    "# Сделаем target переменную: если event_type == 'S1', то 0, если 'S2', то 1\n",
    "df_train['target'] = (df_train['event_type'] == 'S2').astype(int)\n",
    "df_valid['target'] = (df_valid['event_type'] == 'S2').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e908ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_parquet('../../data/classification/processed/crops_500_30_v1_train.parquet', index=False)\n",
    "df_valid.to_parquet('../../data/classification/processed/crops_500_30_v1_valid.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0feb3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(row):\n",
    "    if row['event_type'] == 'S2':\n",
    "        return 0\n",
    "    elif row['event_type'] == 'S1':\n",
    "        if row.get('session_type') == 'fon':\n",
    "            return 1\n",
    "        elif row.get('session_type') == 'own':\n",
    "            return 2\n",
    "        elif row.get('session_type') == 'other':\n",
    "            return 3\n",
    "        else:\n",
    "            return 1  # если ни один из признаков не выставлен, кладём как \"other\" (задайте по необходимости)\n",
    "    else:\n",
    "        return -1  # если вообще не S1/S2\n",
    "\n",
    "df_train['target'] = df_train.apply(make_target, axis=1)\n",
    "df_valid['target'] = df_valid.apply(make_target, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea7744a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_parquet('../../data/classification/processed/crops_500_30_v1_state_train.parquet', index=False)\n",
    "df_valid.to_parquet('../../data/classification/processed/crops_500_30_v1_state_valid.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1ef6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "3:1:1:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57c27b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(target\n",
       " 0    3128\n",
       " 3    1232\n",
       " 1    1169\n",
       " 2    1085\n",
       " Name: count, dtype: int64,\n",
       " target\n",
       " 0    775\n",
       " 3    351\n",
       " 1    275\n",
       " 2    261\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.target.value_counts(), df_valid.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bcc3b3",
   "metadata": {},
   "source": [
    "## Проверка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "51e4528f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of segments: 6614\n",
      "First 3 crop paths:\n",
      " ['/Users/whatislove/study/phd/data/processed/crops_v1_c1_500hz_30ch/Co_y6_016st_otherface_other_S1_0_26124_31124.npy', '/Users/whatislove/study/phd/data/processed/crops_v1_c1_500hz_30ch/Co_y6_016st_otherface_other_S1_0_31124_36123.npy', '/Users/whatislove/study/phd/data/processed/crops_v1_c1_500hz_30ch/Co_y6_016st_otherface_other_S1_0_36123_41123.npy']\n",
      "EEG segments np array shape: (128, 30, 2500)\n",
      "Input torch tensor shape: torch.Size([128, 30, 2500])\n",
      "Model output shape: torch.Size([128, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Загружаем датафрейм с путями к кропам\n",
    "df = pd.read_parquet('../../data/classification/processed/crops_500_30_v1_train.parquet')\n",
    "print(\"Number of segments:\", len(df))\n",
    "print(\"First 3 crop paths:\\n\", df['crop_path'].head(3).tolist())\n",
    "\n",
    "# Загружаем батч из 128 EEG-кропов\n",
    "batch_size = 128\n",
    "batch_paths = df['crop_path'].head(batch_size).tolist()\n",
    "\n",
    "eeg_segments = []\n",
    "expected_n_channels = 30\n",
    "for p in batch_paths:\n",
    "    d = np.load(p)  # shape: (n_channels, n_samples)\n",
    "    if d.shape[0] != expected_n_channels:\n",
    "        raise ValueError(f\"Файл {p} имеет {d.shape[0]} каналов вместо 30!\")\n",
    "    eeg_segments.append(d)\n",
    "eeg_segments = np.stack(eeg_segments)  # (batch_size, 62, n_samples)\n",
    "print(\"EEG segments np array shape:\", eeg_segments.shape)\n",
    "\n",
    "# Преобразуем в torch tensor\n",
    "inputs = torch.from_numpy(eeg_segments.astype(np.float32))  # (batch, 64, samples)\n",
    "print(\"Input torch tensor shape:\", inputs.shape)\n",
    "\n",
    "# При необходимости на cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "n_channels = expected_n_channels\n",
    "in_samples = inputs.shape[2]\n",
    "n_classes = 2  # или другое число классов\n",
    "\n",
    "# Модель EEGNet для 30 каналов\n",
    "class EEGNet(torch.nn.Module):\n",
    "    def __init__(self, n_channels=30, in_samples=2500, n_classes=2):\n",
    "        super(EEGNet, self).__init__()\n",
    "        # Вход: (batch, 64, samples)\n",
    "        self.conv1 = torch.nn.Conv1d(n_channels, 64, kernel_size=7, stride=1, padding=3)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(64)\n",
    "        # groups=64 для depthwise-свёртки\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2, groups=64)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(128)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 256, kernel_size=1)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(256)\n",
    "        self.pool = torch.nn.AdaptiveAvgPool1d(32)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.fc1 = torch.nn.Linear(256 * 32, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, n_classes)\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.nn.functional.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.nn.functional.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = EEGNet(n_channels=n_channels, in_samples=in_samples, n_classes=n_classes).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(inputs)\n",
    "print(\"Model output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb63113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg_clf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
